{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "import pandas as pd\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer as stemmer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk.stem\n",
    "\n",
    "# Special vocabulary module from shoyu\n",
    "import vocabulary_hdp as vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    return [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) for w in doc.translate(str.maketrans('','', string.punctuation)).lower().split(' ')]\n",
    "\n",
    "def rm_stopwords_and_short_words(words):\n",
    "    results = []\n",
    "    for i in words:\n",
    "        if not i in stopwords1 and len(i)  > 3:\n",
    "            results.append(i)\n",
    "    return results\n",
    "\n",
    "def full_preprocess(doc):\n",
    "    return rm_stopwords_and_short_words(preprocess(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrewcarr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andrewcarr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hierarchical_dirichlet/tm_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords1 = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = [full_preprocess(i) for i in df.abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [i for sublist in tokenized_df for i in sublist]\n",
    "all_words =  list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in fewer than 3 abstracts and tokens that appear in more than half the abstracts \n",
    "all_words_counts = np.zeros(len(all_words))\n",
    "\n",
    "for k,i in enumerate(all_words):\n",
    "    for j in tokenized_df:\n",
    "        if i in j:\n",
    "            all_words_counts[k] += 1 \n",
    "            \n",
    "word_counts_dict = list(zip(all_words, list(all_words_counts)))\n",
    "word_counts_dict_ab = list(filter(lambda x: x[1] > 3, word_counts_dict))\n",
    "word_counts_dict_ab2 = list(filter(lambda x: x[1] < len(tokenized_df)/2, word_counts_dict_ab))\n",
    "final_dict = [i[0] for i in word_counts_dict_ab2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df_ab = []\n",
    "for i in tokenized_df:\n",
    "    tokenized_df_ab.append([j for j in i if j in final_dict])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = vocab.Vocabulary()\n",
    "docs = [voca.doc_to_ids(doc) for doc in tokenized_df_ab]\n",
    "beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook data_prep.ipynb to script\n",
      "[NbConvertApp] Writing 2210 bytes to data_prep.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script 'data_prep.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
