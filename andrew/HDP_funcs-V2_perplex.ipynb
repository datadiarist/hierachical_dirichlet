{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import data_preproc\n",
    "from data_preproc import data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, docs = data_preproc(\"../tm_test_data.csv\") # load vocab and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########### SAMPLING T FUNCTIONS ##########\n",
    "##########################################\n",
    "\n",
    "def sampling_t(doc_j, i, word, n_kv, m_k, k_idx): \n",
    "    '''For each word in document j (doc_j), sample for posterior distribution of t and update\n",
    "       table and topic assignments, as well as other count structures within jt_info, n_jtw, m_k, and n_kv\n",
    "       Output: updated doc_j, m_k, n_kv, and topic idx (k_idx)'''\n",
    "    \n",
    "    t_idx = doc_j['w_tbl_idx'][i]\n",
    "    if t_idx + 1 > doc_j['jt_info'].shape[1]:\n",
    "        print(t_idx, doc_j)\n",
    "    k_jt = doc_j['jt_info'][1, t_idx]\n",
    "\n",
    "    ### Remove word if assigned to table (i.e. -x_ji)\n",
    "    if t_idx > 0: \n",
    "        assert k_jt > 0\n",
    "        \n",
    "        doc_j, m_k, k_idx, n_kv = remove_x_ji(doc_j, t_idx, k_jt, m_k, k_idx, word)\n",
    "        \n",
    "    #### Sampling t ####\n",
    "    fk = fk_m_xji(n_kv, word) \n",
    "    \n",
    "    # Un-normalized posterior pvals \n",
    "    t_post = post_pvals_t(doc_j, k_jt, fk, m_k, alpha, gamma)\n",
    "    t_post /=  t_post.sum() \n",
    "    \n",
    "    \n",
    "    # Get most likely table selection\n",
    "    t_samp_idx = np.random.multinomial(1, t_post).argmax()\n",
    "    new_t = doc_j['jt_info'][0, t_samp_idx]  \n",
    "\n",
    "    ## New table is selected\n",
    "    if new_t == 0:\n",
    "\n",
    "        ### Sampling k when t is NEW ###\n",
    "        kt_post = post_pvals_k_new_t(m_k, fk, k_idx, gamma, V)\n",
    "        kt_post /= kt_post.sum()\n",
    "\n",
    "        \n",
    "        # Select most likely topic for new table\n",
    "        kt_samp_idx = np.random.multinomial(1, kt_post).argmax()\n",
    "        new_k = k_idx[kt_samp_idx]\n",
    "\n",
    "        ## New topic selected\n",
    "        if new_k == 0:\n",
    "            \n",
    "            # Create new topic\n",
    "            new_k, n_kv, m_k, k_idx = new_topic(n_kv, m_k, k_idx, beta, V)\n",
    "        \n",
    "        m_k[new_k] +=1 #add to table cnt for topic k\n",
    "        \n",
    "        # Add new table\n",
    "        new_t, doc_j = new_table(new_k, m_k, doc_j, word)\n",
    "\n",
    "    # Assign word to table\n",
    "    doc_j, n_kv = assign_to_table(doc_j, new_t, n_kv, word)\n",
    "    \n",
    "    \n",
    "    return doc_j, n_kv, m_k, k_idx\n",
    "\n",
    "\n",
    "\n",
    "def remove_x_ji(doc_j, t_idx, k_jt, m_k, k_idx, word):\n",
    "    '''Remove word if assigned to table (i.e. -x_ji), calls on remove_table helper function\n",
    "       Inputs: table idx, topic for table t, word\n",
    "       Outputs: updated n_kv, plus additional \n",
    "                 updates on doc_j, m_k (tables in topic k), and k_idx (topics) from remove_table fcn '''\n",
    "    \n",
    "    doc_j['n_jtw'][t_idx][word] -=1  # remove from dictionary table cnt n_jtk\n",
    "    if doc_j['n_jtw'][t_idx][word] == 0:\n",
    "        del doc_j['n_jtw'][t_idx][word]\n",
    "    \n",
    "    doc_j['jt_info'][2, t_idx] -= 1 # remove from table cnt n_jt\n",
    "    n_kv[word, k_jt] -=1 # remove from topic count\n",
    "\n",
    "    # if table is empty, remove table\n",
    "    if doc_j['jt_info'][2, t_idx] == 0: \n",
    "        doc_j, m_k, k_idx = remove_table(t_idx, doc_j, m_k, k_idx)\n",
    "    \n",
    "    return doc_j, m_k, k_idx, n_kv\n",
    "\n",
    "                      \n",
    "def remove_table(t_idx, doc_j, m_k, k_idx):\n",
    "    '''Empty tables (i.e. n_jt == 0) are removed\n",
    "       Inputs: table idx, doc_j and m_k (tables in topic k)\n",
    "       Outputs: Updated doc_j, m_k, k_idx '''\n",
    "    \n",
    "    k_jt = doc_j['jt_info'][1, t_idx]\n",
    "    \n",
    "    # Delete table \n",
    "    #doc_j['jt_info'] = np.delete(doc_j['jt_info'],t_idx, axis =1) # remove table, i.e. del column\n",
    "    doc_j['jt_info'][0,t_idx] = 0\n",
    "    m_k[k_jt] -= 1\n",
    "    \n",
    "    if m_k[k_jt] == 0: \n",
    "        k_idx.remove(k_jt) #if no more tables with topic k, remove topic\n",
    "\n",
    "    return doc_j, m_k, k_idx\n",
    "\n",
    "\n",
    "\n",
    "def fk_m_xji(n_kv, word):\n",
    "    '''Conditional density of x_ji given k and all data items except x_ji'''\n",
    "    return n_kv[word,:] / n_kv.sum(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def post_pvals_t(doc_j, k_jt, fk, m_k, alpha, gamma):\n",
    "    '''Generate posterior pvals for both selecting a new or existing table'''\n",
    "    # if t is NOT NEW\n",
    "    n_jt = doc_j['jt_info'][2,:] # get counts across tables\n",
    "    t_post = n_jt*fk[k_jt]\n",
    "\n",
    "    # if t is NEW\n",
    "    p_xji=0\n",
    "    for k in range(len(k_idx)): # compute p_xji based on paper\n",
    "        p_xji += m_k[k] * fk[k]\n",
    "\n",
    "    p_xji = p_xji + (gamma / V) \n",
    "    t_post[0] = (alpha * p_xji)/ (sum(m_k) + gamma) # t if new store as first\n",
    "    \n",
    "    return t_post\n",
    "\n",
    "\n",
    "def post_pvals_k_new_t(m_k, fk, k_idx, gamma, V):\n",
    "    '''If new table selected, generate posterior pvals for selecting a new or existing topic'''\n",
    "    kt_post = (m_k*fk)[k_idx] # existing topic\n",
    "    kt_post[0] = gamma /V # new topic\n",
    "    \n",
    "    return kt_post\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def new_topic(n_kv, m_k, k_idx, beta, V):\n",
    "    '''If new topic selected, get new topic k and extend structures k_idx (topic idx), n_kv (word-topic matrix), \n",
    "       m_k (tables per topic) for later updates. \n",
    "       Output: new topic and extended structures'''\n",
    "\n",
    "    resize = False\n",
    "    # Create new topic\n",
    "    for idx, k in enumerate(k_idx):\n",
    "        if idx != k: \n",
    "            break\n",
    "        else:\n",
    "            idx = len(k_idx)\n",
    "            if idx >= n_kv.shape[1]:\n",
    "                resize = True\n",
    "            try:\n",
    "                assert idx == k_idx[-1] + 1\n",
    "            except AssertionError as e:\n",
    "                e.args += (idx, k_idx)\n",
    "                raise\n",
    "            \n",
    "    if resize:\n",
    "        n_kv = np.c_[n_kv, np.zeros((V, 1), dtype=int)]\n",
    "        m_k = np.r_[m_k, 0]\n",
    "        assert idx < n_kv.shape[1]\n",
    "    \n",
    "    # Append new topic to list of topics, add column to word-topic matrix, extend table-topic array\n",
    "    k_idx.insert(idx, idx)\n",
    "    n_kv[:, idx] = np.ones(V, dtype = int) * beta\n",
    "    m_k[idx] = 0\n",
    "    \n",
    "    assert idx in k_idx\n",
    "\n",
    "    return idx, n_kv, m_k, k_idx\n",
    "\n",
    "\n",
    "def new_table(new_k, m_k, doc_j, word):\n",
    "    '''If new table selected, get new table idx and extend structures doc_j jt_info and n_jtw for \n",
    "       later updates\n",
    "       Output: new table and extended structures'''\n",
    "    \n",
    "    \n",
    "    resize = False\n",
    "    \n",
    "    for t_idx, t in enumerate(doc_j['jt_info'][0,:]):\n",
    "        if t_idx != t: \n",
    "            break\n",
    "        else:\n",
    "            t_idx = doc_j['jt_info'].shape[1]\n",
    "            resize = True\n",
    "\n",
    "    \n",
    "    if resize:\n",
    "        doc_j['n_jtw'].append({word:0})\n",
    "        doc_j['jt_info'] = np.c_[doc_j['jt_info'], np.zeros((3,1), dtype=int)]\n",
    " \n",
    "    # Add column to doc's 'jt_info' array, set topic of new table,extend discretized word cnt dict\n",
    "    # to allocate word in new table\n",
    "    doc_j['jt_info'][0, t_idx] = t_idx\n",
    "    doc_j['jt_info'][1, t_idx] = new_k\n",
    "    doc_j['n_jtw'][t_idx][word] = 0\n",
    "    \n",
    "    \n",
    "    return t_idx, doc_j\n",
    "\n",
    "\n",
    "\n",
    "def assign_to_table(doc_j, new_t, n_kv, word):\n",
    "    '''Assign word to table new_t with topic new_k in doc_j, add counts to overall table count,  \n",
    "       word-topic matrix and discretized table word counts\n",
    "       Outputs: updated doc_j and n_kv'''\n",
    "    \n",
    "    assert new_t in doc_j['jt_info'][0,:]\n",
    "    # Get word-table assignment idx, add 1 to discretized word cnt dictionary for that table\n",
    "    doc_j['w_tbl_idx'][i] = new_t\n",
    "    doc_j['jt_info'][2, new_t] += 1\n",
    "    \n",
    "    # Get topic of table (either new or old)\n",
    "    new_k = doc_j['jt_info'][1, new_t]\n",
    "    \n",
    "     # Add 1 for word in word-topic count matrix\n",
    "    n_kv[word, new_k] += 1\n",
    "    \n",
    "    # Seat at table, assign corresponding topic, add 1 to overall table count\n",
    "    doc_j['jt_info'][1, new_t] =  new_k\n",
    "    \n",
    "    \n",
    "    doc_j['n_jtw'][new_t].update({word: 1})\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    return doc_j, n_kv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "########### SAMPLING K FUNCTIONS ##########\n",
    "##########################################\n",
    "\n",
    "def sampling_k(doc_j, tbl, n_kv, m_k, k_idx):\n",
    "    '''For each TABLE in document j (doc_j), sample for posterior distribution of k and update\n",
    "       table and topic assignments, as well as other count structures within jt_info, n_jtw, m_k, and n_kv\n",
    "       Output: updated doc_j, m_k, n_kv, and topic idx (k_idx)'''\n",
    "    \n",
    "    #### START of Sampling k loop through tables, (skip first index always, 0 = dummy idx) ####\n",
    "    if tbl != 0: \n",
    "\n",
    "        # Get topic k, remove all components from table t associated with topic k\n",
    "        doc_j, m_k, k_idx = remove_Xvec_ji(doc_j, tbl, m_k, k_idx)\n",
    "\n",
    "        # Samples posterior p-vals K\n",
    "        post_k = post_pvals_k(doc_j, tbl, n_kv, m_k, k_idx, V, beta)\n",
    "        post_k /= post_k.sum()\n",
    "        \n",
    "        # Select most likely topic for table\n",
    "        k_samp_idx = np.random.multinomial(1, post_k).argmax()\n",
    "        \n",
    "        new_k = k_idx[k_samp_idx]\n",
    "\n",
    "        ## New topic selected\n",
    "        if new_k == 0:\n",
    "\n",
    "            # Create new topic\n",
    "            new_k, n_kv, m_k, k_idx = new_topic(n_kv, m_k, k_idx, beta, V)\n",
    "        \n",
    "        # Add table to topic k count\n",
    "        m_k[new_k] += 1\n",
    "        \n",
    "        doc_j, n_kv = rearranging_k_counts(doc_j, tbl, new_k, n_kv)\n",
    "            \n",
    "\n",
    "    return doc_j, n_kv, m_k, k_idx \n",
    "\n",
    "\n",
    "\n",
    "def remove_Xvec_ji(doc_j, tbl, m_k, k_idx):\n",
    "    '''Remove table from topic k (i.e. related removing all components associated to table t later)\n",
    "       If table becomes empty, remove topic'''\n",
    "    \n",
    "    # Get topic k, remove all components from table t associated with topic k\n",
    "    k_jt = doc_j['jt_info'][1, tbl]\n",
    "    m_k[k_jt] -= 1 # remove from table-topic vector\n",
    "\n",
    "    if m_k[k_jt] == 0:\n",
    "        k_idx.remove(k_jt) # if no more tables with topic k, remove topic k and set table's topic to 0\n",
    "        doc_j['jt_info'][1, tbl] = 0\n",
    "    \n",
    "    return doc_j, m_k, k_idx\n",
    "\n",
    "\n",
    "def post_pvals_k(doc_j, tbl, n_kv, m_k, k_idx, V, beta):\n",
    "    '''Compute explicit posterior pvals distribution based on dirichlet-multinomial form'''\n",
    "    \n",
    "    # Topic k of table t\n",
    "    k_jt = doc_j['jt_info'][1, tbl]\n",
    "\n",
    "    # Remove all counts associated with topic k in table t, from overall topic counts (n_k)\n",
    "    \n",
    "    n_kv = n_kv.copy() #### NOTE: fix, remind me to never disregard what Cliburn says in class 20 times about slicing\n",
    "    n_k = n_kv.sum(axis = 0)\n",
    "    n_jt = doc_j['jt_info'][2, tbl]\n",
    "    n_k[k_jt] -= n_jt\n",
    "    n_k = n_k[k_idx]\n",
    "\n",
    "    # Initialized k posterior in log-form for simplicity, this computes f_k^{-X_ji} \n",
    "    # has Dirichlet-Multinomial form\n",
    "    log_post_k = np.log(m_k[k_idx]) + gammaln(n_k) - gammaln(n_k + n_jt)\n",
    "    log_post_k_new = np.log(gamma) + gammaln(V*beta) - gammaln((V*beta) + n_jt)\n",
    "\n",
    "    # Remove individual word counts associated with topic k\n",
    "    # add their contributions to k posterior\n",
    "    for w_key, w_cnt in doc_j['n_jtw'][tbl].items():\n",
    "\n",
    "        assert w_cnt >= 0\n",
    "        if w_cnt == 0: # if word count is 0 skip\n",
    "            continue\n",
    "\n",
    "        # For word w, get counts across topics - if zero set as beta\n",
    "        w_cnt_k = n_kv[w_key, :]\n",
    "        w_cnt_k[w_cnt_k == 0] = beta\n",
    "        \n",
    "        if np.any(w_cnt_k <= 0): print(\"pre- check\", j, tbl, k_jt, w_key, w_cnt_k, w_cnt)\n",
    "\n",
    "        # For specific topic k, remove count from associated table t\n",
    "        w_cnt_k[k_jt] -= w_cnt\n",
    "        w_cnt_k = w_cnt_k[k_idx]\n",
    "\n",
    "        w_cnt_k[0] = 1\n",
    "        if np.any(w_cnt_k <= 0): print(\"check\", j, tbl, k_jt, w_key, w_cnt_k, w_cnt)\n",
    "\n",
    "        # Add contributions\n",
    "        log_post_k += gammaln(w_cnt_k  + w_cnt) - gammaln(w_cnt_k)\n",
    "        log_post_k_new += gammaln(beta + w_cnt) - gammaln(beta)\n",
    "\n",
    "\n",
    "    # set K new\n",
    "    log_post_k[0] = log_post_k_new\n",
    "\n",
    "    # Bring back to non-log realm, normalize k-posterior \n",
    "    post_k = np.exp(log_post_k - log_post_k.max())\n",
    "\n",
    "    return post_k\n",
    "\n",
    "\n",
    "def rearranging_k_counts(doc_j, tbl, new_k, n_kv):\n",
    "    '''For sampled k, rearrange counts for topics accordingly (i.e. if a new k was selected, subtract\n",
    "       from previous k and add to new k in word-topic matrix)'''\n",
    "    # If new topic for table t is selected, set topic to new topic\n",
    "    k_jt = doc_j['jt_info'][1, tbl]\n",
    "    if new_k != k_jt: \n",
    "        doc_j['jt_info'][1, tbl] = new_k\n",
    "\n",
    "        # On word-topic matrix, move counts from old topic to new topic\n",
    "        for k, cnt in doc_j['n_jtw'][tbl].items():\n",
    "            if k_jt != 0: \n",
    "                n_kv[k, k_jt] -= cnt\n",
    "\n",
    "            n_kv[k, new_k] += cnt\n",
    "            \n",
    "    return doc_j, n_kv\n",
    "   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Initialize params ######\n",
    "#############################\n",
    "\n",
    "# Hyper params\n",
    "beta = 0.5 # word concentration (LDA)\n",
    "alpha = np.random.gamma(1, 1) # GP hyperparam\n",
    "gamma = np.random.gamma(1, 1) # Base GP hyperparam\n",
    "\n",
    "V = vocab.shape[0] # length of vocabulary\n",
    "\n",
    "D = len(docs) # numb docs\n",
    "\n",
    "\n",
    "#### Storing structures\n",
    "\n",
    "# dictionary per doc j, \n",
    "# has a 'jt_info' 3 x k_jt array where \n",
    "# 1st row = table idx (using_t), 2nd row = topic idx (k_jt), 3rd row = table cnt (n_jt)\n",
    "# \n",
    "# 'w_tbl_idx' is a vector storing word-table assignments (t_ji)\n",
    "# \n",
    "# 'n_jtw' is cnt dict discretized by words within each topic-table (circle) \n",
    "\n",
    "docs_dict = {j:{'jt_info':np.zeros((3,1), dtype=int, order='F'), \n",
    "               'w_tbl_idx': np.zeros(len(docs[j]), dtype=int) -1, \n",
    "               'n_jtw':[{beta:beta}]} for j in range(D)}\n",
    "\n",
    "\n",
    "\n",
    "# A V+1 x k matrix, each row is a word so column sums gives us n_k\n",
    "n_kv = np.ones((V, 1)) * beta\n",
    "\n",
    "m_k = np.ones(1, dtype=int) # 1 x k matrix storing tables per topic\n",
    "\n",
    "k_idx = [0] # list storing topics\n",
    "\n",
    "x_ji = docs # data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################\n",
    "#### HDP ######\n",
    "#############################\n",
    "for z in range(5):\n",
    "    for j, x_i in enumerate(x_ji):\n",
    "\n",
    "        doc_j = docs_dict[j]\n",
    "\n",
    "        for i, w in enumerate(x_i):\n",
    "\n",
    "            doc_j, n_kv, m_k, k_idx = sampling_t(doc_j, i, w, n_kv, m_k, k_idx)\n",
    "\n",
    "        docs_dict[j] = doc_j\n",
    "\n",
    "\n",
    "    for j in range(D):\n",
    "        doc_j = docs_dict[j]\n",
    "        for tbl in doc_j['jt_info'][0, :]:\n",
    "\n",
    "            doc_j, n_kv, m_k, k_idx = sampling_k(doc_j, tbl, n_kv, m_k, k_idx)\n",
    "\n",
    "        docs_dict[j] = doc_j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(dicts):\n",
    "    \n",
    "    for d in dicts.values():\n",
    "\n",
    "        idxs = np.argwhere(d['jt_info'][2, :] == 0).ravel()[1:]\n",
    "\n",
    "        d['jt_info'] = np.delete(d['jt_info'], idxs, axis =1)\n",
    "\n",
    "        for i in sorted(idxs, reverse = True):  \n",
    "            d['n_jtw'].pop(i)\n",
    "    \n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = clean_up(docs_dict) # final dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
