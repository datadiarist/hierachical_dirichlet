{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import data_preproc\n",
    "from data_preproc import data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca, docs = data_preproc(\"tm_test_data.csv\") # load vocab and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special class \n",
    "class DefaultDict(dict):\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "        dict.__init__(self)\n",
    "    def __getitem__(self, k):\n",
    "        return dict.__getitem__(self, k) if k in self else self.v\n",
    "    def update(self, d):\n",
    "        dict.update(self, d)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing default values for start of alg\n",
    "\n",
    "# Hyperparameters (concentration parms of DP distributions)\n",
    "gamma = np.random.gamma(1, 1)\n",
    "alpha = np.random.gamma(1, 1)\n",
    "beta = .5\n",
    "\n",
    "# size of vocabulary \n",
    "V = len(voca)\n",
    "# To see words type voca.vocas\n",
    "\n",
    "# Number of documents \n",
    "M = len(docs)\n",
    "\n",
    "# Table index for document j\n",
    "using_t = [[0] for j in range(M)]\n",
    "\n",
    "# Dish index - 0 means draw a new topic \n",
    "k = 0\n",
    "using_k = [0]\n",
    "\n",
    "\n",
    "# x is data, t is table index, k is topic index, n is number of terms, m is number of tables\n",
    "\n",
    "# Vocabulary for each doc-term - this is the input data and doesn't change \n",
    "x_ji = docs\n",
    "\n",
    "# Topics of document and table\n",
    "k_jt = [np.zeros(1 ,dtype=int) for j in range(M)]\n",
    "\n",
    "# Number of terms for each table of document\n",
    "n_jt = [np.zeros(1 ,dtype=int) for j in range(M)]   \n",
    "\n",
    "# Number of terms for each table and vocabulary of document \n",
    "n_jtv = [[None] for j in range(M)]\n",
    "\n",
    "\n",
    "m = 0\n",
    "# Number of tables for each topic\n",
    "m_k = np.ones(1 ,dtype=int)  \n",
    "\n",
    "# Number of terms for each topic ( + beta * V )\n",
    "n_k = np.array([beta * V]) \n",
    "\n",
    "# Number of terms for each topic and vocabulary ( + beta )\n",
    "n_kv = [DefaultDict(0)]            \n",
    "\n",
    "# Table for each document and term (-1 means not-assigned)\n",
    "t_ji = [np.zeros(len(x_i), dtype=int) - 1 for x_i in docs]\n",
    "\n",
    "gamma = .0953558363500576\n",
    "alpha = 0.9055762536667756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers ## \n",
    "\n",
    "# Function that takes v (term index) and returns a list that represents the distribution of a term across topics -- i.e. each element is the proportion of terms in topic k that are term v \n",
    "def calc_f_k(v):\n",
    "    return [n_kv[v] for n_kv in n_kv]/n_k\n",
    "\n",
    "\n",
    "# Function that calculates the posterior distribution of tables for doc j / arguments: j - doc index,f_k - distribution of term across topics \n",
    "\n",
    "def calc_table_posterior(j, f_k, using_t, n_jt):\n",
    "    \n",
    "    # Store list of tables for doc j as using_t\n",
    "    using_t = using_t[j]\n",
    "    \n",
    "    # Number of terms in doc j at each table times disibutrion of terms across topics ## CHECK THIS \n",
    "    p_t = n_jt[j][using_t] * f_k[k_jt[j][using_t]]\n",
    "    \n",
    "    # Sum of number of tables across topics weighted by f_k + gamma/(vocab size) -- this corresponds with the probability of selecting a new table \n",
    "    p_x_ji = np.inner(m_k, f_k) + gamma / V\n",
    "    \n",
    "    # Storing probability of new table as first element \n",
    "    p_t[0] = p_x_ji * alpha / (gamma + m)\n",
    "\n",
    "    # Return likelihood over prior \n",
    "    return p_t / p_t.sum()\n",
    "\n",
    "\n",
    "def calc_dish_posterior_w(f_k):\n",
    "    \"calculate dish(topic) posterior when one word is removed\"\n",
    "    \n",
    "    p_k = (m_k * f_k)[using_k]\n",
    "    p_k[0] = gamma / V\n",
    "    \n",
    "    return p_k / p_k.sum()\n",
    "    \n",
    "    \n",
    "def calc_dish_posterior_t(j, t, n_k, n_jt, n_jtv):\n",
    "    \"calculate dish(topic) posterior when one table is removed\"\n",
    "    k_old = k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "    \n",
    "    Vbeta = V * beta\n",
    "    n_k = n_k.copy()\n",
    "    n_jt2 = n_jt.copy()[j][t]\n",
    "    n_k[k_old] -= n_jt2\n",
    "    n_k = n_k[using_k]\n",
    "    log_p_k = np.log(m_k[using_k]) + gammaln(n_k) - gammaln(n_k + n_jt2)\n",
    "    log_p_k_new = np.log(gamma) + gammaln(Vbeta) - gammaln(Vbeta + n_jt2)\n",
    "\n",
    "    gammaln_beta = gammaln(beta)\n",
    "    for w, n_jtw in n_jtv[j][t].items():\n",
    "        assert n_jtw >= 0\n",
    "        if n_jtw == 0: continue\n",
    "        n_kw = np.array([n.get(w, beta) for n in n_kv])\n",
    "        n_kw[k_old] -= n_jtw\n",
    "        n_kw = n_kw[using_k]\n",
    "        n_kw[0] = 1 # dummy for logarithm's warning\n",
    "        if np.any(n_kw <= 0): print(n_kw) # for debug\n",
    "        log_p_k += gammaln(n_kw + n_jtw) - gammaln(n_kw)\n",
    "        log_p_k_new += gammaln(beta + n_jtw) - gammaln_beta\n",
    "        \n",
    "        \n",
    "    log_p_k[0] = log_p_k_new\n",
    "    \n",
    "    p_k = np.exp(log_p_k - log_p_k.max())\n",
    "    return p_k / p_k.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPLDA Alg ### \n",
    "np.random.seed(123)\n",
    "eval_score = []\n",
    "\n",
    "# g = 5 epochs\n",
    "for g in range(15):\n",
    "    \n",
    "# Loop - sampling_t - j is doc index (e.g. first doc is 0), i is term index (0 is first element of global vocabulary voca.vocas)\n",
    "    eval_score.append(perplexity(V, x_ji, m_k, gamma, alpha, using_k, k_jt, n_jt, beta, n_kv, n_k))\n",
    "    \n",
    "    # Loop through the data \n",
    "    for j, x_i in enumerate(x_ji):\n",
    "        \n",
    "        # For each doc, loop through each term\n",
    "        for i in range(len(x_i)):\n",
    "            \n",
    "            ### Reassign table for term i in document j ###\n",
    "            t = t_ji[j][i]\n",
    "            if t  > 0:\n",
    "                k = k_jt[j][t]\n",
    "                assert k > 0\n",
    "        \n",
    "                # decrease counters\n",
    "                v = x_ji[j][i]\n",
    "                n_kv[k][v] -= 1\n",
    "                n_k[k] -= 1\n",
    "                n_jt[j][t] -= 1\n",
    "                n_jtv[j][t][v] -= 1\n",
    "        \n",
    "                if n_jt[j][t] == 0:\n",
    "                    \n",
    "                    # Remove table \n",
    "                    \n",
    "                    # Set topic index at doc j and table t to k\n",
    "                    k = k_jt[j][t]\n",
    "                    \n",
    "                    # Remove t from list of tables being used in doc j\n",
    "                    using_t[j].remove(t)\n",
    "                    \n",
    "                    # Decrease number of tables for topic k by 1\n",
    "                    m_k[k] -= 1\n",
    "                    # Decrease number of tables overall (?) by 1\n",
    "                    m -= 1\n",
    "                    assert m_k[k] >= 0\n",
    "                    \n",
    "                    # If number of tables for topic k is 0 remove topic\n",
    "                    if m_k[k] == 0:\n",
    "                        using_k.remove(k)\n",
    "        \n",
    "                                    \n",
    "            # Store term index as v\n",
    "            v = x_ji[j][i]\n",
    "            \n",
    "            # Calculate the distribution of v across the topics -- f_k will be the base distribution for the calc_table_posterior function \n",
    "            f_k = calc_f_k(v)\n",
    "            assert f_k[0] == 0 # f_k[0] is a dummy and will be erased\n",
    "        \n",
    "            \n",
    "            # Calculating the posterior distribution of tables --  p(t_ji=t)\n",
    "            p_t = calc_table_posterior(j, f_k, using_t, n_jt)\n",
    "            \n",
    "            \n",
    "            # This just prints some results while the alg runs - blocking out for now     \n",
    "            # if len(p_t) > 1 and p_t[1] < 0: dump()\n",
    "                \n",
    "            # Sample from the posterior and assigned the corresponding table index to t_new (not necessarily a new table - it's a new sample)\n",
    "            t_new = using_t[j][np.random.multinomial(1, p_t).argmax()]\n",
    "            \n",
    "            # If t_new == 0 (i.e. the table is new)\n",
    "            if t_new == 0:\n",
    "                \n",
    "                # Calculate the posterior distribution of topics \n",
    "                p_k = calc_dish_posterior_w(f_k)\n",
    "                \n",
    "                # Sample from this posterior distribution and assign the corresponding topic index to k_new \n",
    "                k_new = using_k[np.random.multinomial(1, p_k).argmax()]\n",
    "                \n",
    "                # If k_new == 0 (i.e. the topic is new)\n",
    "                if k_new == 0:\n",
    "                    \n",
    "                    # Add new dish and store as k_new \n",
    "                    for k_new, k in enumerate(using_k):\n",
    "                        if k_new != k: break\n",
    "                    else:\n",
    "                        k_new = len(using_k)\n",
    "                        if k_new >= len(n_kv):\n",
    "                            n_k = np.resize(n_k, k_new + 1)\n",
    "                            m_k = np.resize(m_k, k_new + 1)\n",
    "                            n_kv.append(None)\n",
    "                        assert k_new == using_k[-1] + 1\n",
    "                        assert k_new < len(n_kv)\n",
    "    \n",
    "                    using_k.insert(k_new, k_new)\n",
    "                    n_k[k_new] = beta * V\n",
    "                    m_k[k_new] = 0\n",
    "                    n_kv[k_new] = DefaultDict(beta)\n",
    "                    \n",
    "                assert k_new in using_k\n",
    "                \n",
    "                for t_new, t in enumerate(using_t[j]):\n",
    "                    if t_new != t: break\n",
    "                else:\n",
    "                    t_new = len(using_t[j])\n",
    "                    n_jt[j].resize(t_new+1)\n",
    "                    k_jt[j].resize(t_new+1)\n",
    "                    n_jtv[j].append(None)\n",
    "            \n",
    "                using_t[j].insert(t_new, t_new)\n",
    "                n_jt[j][t_new] = 0  # to make sure\n",
    "                n_jtv[j][t_new] = DefaultDict(0)\n",
    "            \n",
    "                k_jt[j][t_new] = k_new\n",
    "                \n",
    "                m_k[k_new] += 1\n",
    "                \n",
    "                m += 1\n",
    "            \n",
    "            assert t_new in using_t[j]\n",
    "            t_ji[j][i] = t_new\n",
    "            n_jt[j][t_new] += 1\n",
    "    \n",
    "            k_new = k_jt[j][t_new]\n",
    "            n_k[k_new] += 1\n",
    "    \n",
    "            v = x_ji[j][i]\n",
    "            n_kv[k_new][v] += 1\n",
    "            n_jtv[j][t_new][v] += 1\n",
    "            \n",
    "                \n",
    "    for j in range(M):\n",
    "        for t in using_t[j]:\n",
    "            if t != 0: \n",
    "                \"\"\"sampling k (dish=topic) from posterior\"\"\"\n",
    "    \n",
    "                #This makes the table leave from its dish and only the table counter decrease. The word counters (n_k and n_kv) stay.\n",
    "                \n",
    "                k = k_jt[j][t]\n",
    "                assert k > 0\n",
    "                assert m_k[k] > 0\n",
    "                \n",
    "                m_k[k] -= 1\n",
    "                m -= 1\n",
    "                if m_k[k] == 0:\n",
    "                    using_k.remove(k)\n",
    "                    k_jt[j][t] = 0\n",
    "                #\n",
    "                    \n",
    "                # sampling of k\n",
    "                p_k = calc_dish_posterior_t(j, t, n_k, n_jt, n_jtv)\n",
    "                \n",
    "                k_new = using_k[np.random.multinomial(1, p_k).argmax()]\n",
    "                \n",
    "#\n",
    "                if k_new == 0:\n",
    "                    # Add new dish  \n",
    "                    for k_new, k in enumerate(using_k):\n",
    "                        if k_new != k: break\n",
    "                    else:\n",
    "                        k_new = len(using_k)\n",
    "                        if k_new >= len(n_kv):\n",
    "                            n_k = np.resize(n_k, k_new + 1)\n",
    "                            m_k = np.resize(m_k, k_new + 1)\n",
    "                            n_kv.append(None)\n",
    "                        assert k_new == using_k[-1] + 1\n",
    "                        assert k_new < len(n_kv)\n",
    "                \n",
    "                    using_k.insert(k_new, k_new)\n",
    "                    n_k[k_new] = beta * V\n",
    "                    m_k[k_new] = 0\n",
    "                    n_kv[k_new] = DefaultDict(beta)\n",
    "                    \n",
    "      \n",
    "                    \n",
    "                m += 1\n",
    "                m_k[k_new] += 1\n",
    "            \n",
    "                k_old = k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "                if k_new != k_old:\n",
    "                    k_jt[j][t] = k_new\n",
    "            \n",
    "                    n_jt2 = n_jt.copy()[j][t]\n",
    "                    if k_old != 0: n_k[k_old] -= n_jt2\n",
    "                    n_k[k_new] += n_jt2\n",
    "                    for v, n in n_jtv[j][t].items():\n",
    "                        if k_old != 0: n_kv[k_old][v] -= n\n",
    "                        n_kv[k_new][v] += n\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_k)-1\n",
    "\n",
    "## This is the same number of topics you get from 5 iterations of the original alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worddist(beta, n_kv, n_k, using_k):\n",
    "        \"\"\"return topic-word distribution without new topic\"\"\"\n",
    "        return [DefaultDict(beta / n_k[k]).update(\n",
    "            (v, n_kv / n_k[k]) for v, n_kv in n_kv[k].items())\n",
    "                for k in using_k if k != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(worddist(beta, n_kv, n_k, using_k)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq59 = n_kv[1].get(59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_words = n_k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004891648713724496"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word dist tells you the distribution of words in each topic \n",
    "(freq59/tot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about docdist ??\n",
    "\n",
    "def docdist(m_k, gamma, alpha, using_k, k_jt, n_jt):\n",
    "    \"\"\"return document-topic distribution with new topic\"\"\"\n",
    "\n",
    "    # am_k = effect from table-dish assignment\n",
    "    am_k = np.array(m_k, dtype=float)\n",
    "    am_k[0] = gamma\n",
    "    am_k *= alpha / am_k[using_k].sum()\n",
    "\n",
    "    theta = []\n",
    "    for j, n_jt in enumerate(n_jt):\n",
    "        p_jk = am_k.copy()\n",
    "        for t in using_t[j]:\n",
    "            if t == 0: continue\n",
    "            k = k_jt[j][t]\n",
    "            p_jk[k] += n_jt[t]\n",
    "        p_jk = p_jk[using_k]\n",
    "        theta.append(p_jk / p_jk.sum())\n",
    "\n",
    "    return np.array(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.38864933e-07, 9.18329987e-01, 8.16072863e-02, 6.19880197e-05])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns doc (622) length array where each element is a list giving the distribution of words in the doc across topics (index 0 is empty topic but not 0 (almost 0 -- bad words?))\n",
    "docdist(m_k, gamma, alpha, using_k, k_jt, n_jt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity constructs likelihoods from distribution of words within topics and distribution of topics across documents \n",
    "def perplexity(V, x_ji, m_k, gamma, alpha, using_k, k_jt, n_jt, beta, n_kv, n_k):\n",
    "    \n",
    "    # This just stores word-topic distribution in phi \n",
    "    phi = [DefaultDict(1.0/V)] + worddist(beta, n_kv, n_k, using_k)\n",
    "    \n",
    "    # This stores doc-topic distribution in theta\n",
    "    theta = docdist(m_k, gamma, alpha, using_k, k_jt, n_jt)\n",
    "    \n",
    "    # Init log-likelihood at 0\n",
    "    log_likelihood = 0\n",
    "    \n",
    "    # N is number of words in a doc\n",
    "    N = 0\n",
    "    \n",
    "    # Looping through docs and its doc-topic distribution\n",
    "    for x_ji, p_jk in zip(x_ji, theta):\n",
    "        \n",
    "        # Looping through words in doc\n",
    "        for v in x_ji:\n",
    "            \n",
    "            # word_prob is the sum of the products of probs a doc appears in each of the topics (# topic length vector) and the probs a word appears in each of the topics (also # topic length)\n",
    "            # So we're summing across topics \n",
    "            word_prob = sum(p * p_kv[v] for p, p_kv in zip(p_jk, phi))\n",
    "            \n",
    "            # Then log it and subtract that from 0 (is negative LL)\n",
    "            log_likelihood -= np.log(word_prob)\n",
    "            \n",
    "        # Number of words in the doc    \n",
    "        N += len(x_ji)\n",
    "        \n",
    "    # Return exp(-LL/N)\n",
    "    return np.exp(log_likelihood / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712.2890466460494"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want a really big likelihood -- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.38864933e-07, 9.18329987e-01, 8.16072863e-02, 6.19880197e-05])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docdist(m_k, gamma, alpha, using_k, k_jt, n_jt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1560.999999995949,\n",
       " 750.9003116275189,\n",
       " 742.400810395608,\n",
       " 738.8001839669652,\n",
       " 736.4411240411055,\n",
       " 733.6373348926699,\n",
       " 731.0145012225074,\n",
       " 730.503458690267,\n",
       " 727.7395820200178,\n",
       " 725.1762116580434,\n",
       " 722.3193194874614,\n",
       " 719.9970685605019,\n",
       " 717.9823719365794,\n",
       " 715.6751465404237,\n",
       " 714.8331588297295]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
