{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer as stemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stemmer = stemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    '''Function that lemmatizes words in abstract by verbs'''\n",
    "    \n",
    "    return [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) \n",
    "            for w in doc.translate(str.maketrans('','', string.punctuation)).lower().split(' ')]\n",
    "\n",
    "\n",
    "def rm_stopwords_and_short_words(words, st_words):\n",
    "    '''Function removes stop words and those with length < 3'''\n",
    "    results = []\n",
    "    for i in words:\n",
    "        if not i in st_words and len(i)  > 3:\n",
    "            results.append(i)\n",
    "    return results\n",
    "\n",
    "def full_preprocess(doc, st_words):\n",
    "    '''Performs word lemmatization and stopword removal'''\n",
    "    return rm_stopwords_and_short_words(preprocess(doc), st_words)\n",
    "\n",
    "\n",
    "def tf(docs, st_words):\n",
    "    '''Term frequency matrix function, calculates the term frequencies of word from an text-document paired dictionary input. \n",
    "       The output is a term frequency table '''\n",
    "    \n",
    "    # generate counts per document\n",
    "    counts = {k: Counter(full_preprocess(txt, st_words)) for k, txt in docs.items()}\n",
    "    tf_df = pd.DataFrame.from_dict(counts).fillna(0).astype(int) # build pandas df, fill empty vals with 0s\n",
    "    \n",
    "    return(tf_df)\n",
    "\n",
    "\n",
    "def token_filtering(tf_df):\n",
    "    '''Filters out tokens that appear in fewer than 3 abstracts and tokens that appear in more than half the abstracts '''\n",
    "    filtered_df = tf_df[(tf_df.sum(axis=1) > 3)]\n",
    "    filtered_df = filtered_df[(filtered_df.astype(bool).sum(axis=1) / tf_df.shape[1] < 0.5)]\n",
    "    \n",
    "    return filtered_df\n",
    "    \n",
    "def get_docs(df):\n",
    "    '''quickly build a dictionary based on filtered dataframe, get words w/ unique ids'''\n",
    "    df.reset_index(inplace=True)\n",
    "    filt_words = pd.DataFrame.to_dict(df.drop(columns='index'))\n",
    "    \n",
    "    return [[word for word, cnt in words.items() if cnt!=0] for dkeys, words in filt_words.items()]\n",
    "    \n",
    "    \n",
    "def data_preproc(file_path):\n",
    "    '''Data pre-processing function\n",
    "       Input -> url to data in CSV format where each row is a document text'''\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    in_docs = {k: str(txt[0]) for k,txt in enumerate(df.values)}\n",
    "    \n",
    "    st_words = stopwords.words('english')\n",
    "    \n",
    "    tf_df = tf(in_docs, st_words)\n",
    "    \n",
    "    filtered_df = token_filtering(tf_df)\n",
    "    \n",
    "    vocab = filtered_df.index.values\n",
    "        \n",
    "    docs = get_docs(filtered_df)\n",
    "    \n",
    "    return [vocab, docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook data_preproc.ipynb to script\n",
      "[NbConvertApp] Writing 2771 bytes to data_preproc.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script 'data_preproc.ipynb'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
