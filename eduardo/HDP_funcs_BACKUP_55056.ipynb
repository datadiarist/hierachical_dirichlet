{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import data_preproc\n",
    "from data_preproc import data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, docs = data_preproc(\"../tm_test_data.csv\") # load vocab and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Initialize params ######\n",
    "#############################\n",
    "\n",
    "# Hyper params\n",
    "beta = 0.5 # word concentration (LDA)\n",
    "alpha = np.random.gamma(1, 1) # GP hyperparam\n",
    "gamma = np.random.gamma(1, 1) # Base GP hyperparam\n",
    "\n",
    "V = vocab.shape[0] # length of vocabulary\n",
    "\n",
    "D = len(docs) # numb docs\n",
    "\n",
    "\n",
    "#### Storing structures\n",
    "\n",
    "# dictionary per doc j, \n",
    "# has a 'jt_info' 3 x k_jt array where \n",
    "# 1st row = table idx (using_t), 2nd row = topic idx (k_jt), 3rd row = table cnt (n_jt)\n",
    "# \n",
    "# 'w_tbl_idx' is a vector storing word-table assignments (t_ji)\n",
    "# \n",
    "# 'n_jtw' is cnt dict discretized by words within each topic-table (circle) \n",
    "\n",
    "docs_dict = {j:{'jt_info':np.zeros((3,1), dtype=int), \n",
    "               'w_tbl_idx': np.zeros(len(docs[j]), dtype=int) -1, \n",
    "               'n_jtw':[{beta:beta}]} for j in range(D)}\n",
    "\n",
    "\n",
    "\n",
    "# A V+1 x k matrix, each row is a word so column sums gives us n_k\n",
    "n_kv = np.ones((V, 1)) * beta\n",
    "\n",
    "m_k = np.ones(1, dtype=int) # 1 x k matrix storing tables per topic\n",
    "\n",
    "k_idx = [0] # list storing topics\n",
    "\n",
    "x_ji = docs # data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########### SAMPLING T FUNCTIONS ##########\n",
    "##########################################\n",
    "\n",
    "\n",
    "def sampling_t(doc_j, i, word, n_kv, m_k, k_idx): \n",
    "    '''For each word in document j (doc_j), sample for posterior distribution of t and update\n",
    "       table and topic assignments, as well as other count structures within jt_info, n_jtw, m_k, and n_kv\n",
    "       Output: updated doc_j, m_k, n_kv, and topic idx (k_idx)'''\n",
    "    \n",
    "    t_idx = doc_j['w_tbl_idx'][i]\n",
    "    if t_idx + 1 > doc_j['jt_info'].shape[1]:\n",
    "        print(t_idx, doc_j)\n",
    "    k_jt = doc_j['jt_info'][1, t_idx]\n",
    "\n",
    "    ### Remove word if assigned to table (i.e. -x_ji)\n",
    "    if t_idx > 0: \n",
    "        assert k_jt > 0\n",
    "        \n",
    "        doc_j, m_k, k_idx, n_kv = remove_x_ji(doc_j, t_idx, k_jt, m_k, k_idx, word)\n",
    "        \n",
    "    #### Sampling t ####\n",
    "    fk = fk_m_xji(n_kv, word) \n",
    "    \n",
    "    # Un-normalized posterior pvals \n",
    "    t_post = post_pvals_t(doc_j, k_jt, fk, m_k, alpha, gamma)\n",
    "    t_post /=  t_post.sum() \n",
    "    \n",
    "    \n",
    "    # Get most likely table selection\n",
    "    t_samp_idx = np.random.multinomial(1, t_post).argmax()\n",
    "    new_t = doc_j['jt_info'][0, t_samp_idx]  \n",
    "\n",
    "    ## New table is selected\n",
    "    if new_t == 0:\n",
    "\n",
    "        ### Sampling k when t is NEW ###\n",
    "        kt_post = post_pvals_k_new_t(m_k, fk, k_idx, gamma, V)\n",
    "        if any(kt_post <0):\n",
    "            print(n_kv[word,:], m_k, fk,kt_post)\n",
    "            \n",
    "        kt_post /= kt_post.sum()\n",
    "\n",
    "        \n",
    "        # Select most likely topic for new table\n",
    "        kt_samp_idx = np.random.multinomial(1, kt_post).argmax()\n",
    "        new_k = k_idx[kt_samp_idx]\n",
    "\n",
    "        ## New topic selected\n",
    "        if new_k == 0:\n",
    "            \n",
    "            # Create new topic\n",
    "            new_k, n_kv, m_k, k_idx = new_topic(n_kv, m_k, k_idx, beta, V)\n",
    "        \n",
    "        m_k[new_k] +=1 #add to table cnt for topic k\n",
    "        \n",
    "        # Add new table\n",
    "        new_t, doc_j = new_table(new_k, m_k, doc_j, word)\n",
    "\n",
    "    # Assign word to table\n",
    "    doc_j, n_kv = assign_to_table(doc_j, new_t, n_kv, word)\n",
    "    \n",
    "    return doc_j, n_kv, m_k, k_idx\n",
    "\n",
    "\n",
    "\n",
    "def remove_x_ji(doc_j, t_idx, k_jt, m_k, k_idx, word):\n",
    "    '''Remove word if assigned to table (i.e. -x_ji), calls on remove_table helper function\n",
    "       Inputs: table idx, topic for table t, word\n",
    "       Outputs: updated n_kv, plus additional \n",
    "                 updates on doc_j, m_k (tables in topic k), and k_idx (topics) from remove_table fcn '''\n",
    "    \n",
    "    doc_j['n_jtw'][t_idx][word] -=1  # remove from dictionary table cnt n_jtk\n",
    "    doc_j['jt_info'][2, t_idx] -= 1 # remove from table cnt n_jt\n",
    "    n_kv[word, k_jt] -=1 # remove from topic count\n",
    "\n",
    "    # if table is empty, remove table\n",
    "    if doc_j['jt_info'][2, t_idx] == 0: \n",
    "        doc_j, m_k, k_idx = remove_table(t_idx, doc_j, m_k, k_idx)\n",
    "    \n",
    "    return doc_j, m_k, k_idx, n_kv\n",
    "\n",
    "                      \n",
    "def remove_table(t_idx, doc_j, m_k, k_idx):\n",
    "    '''Empty tables (i.e. n_jt == 0) are removed\n",
    "       Inputs: table idx, doc_j and m_k (tables in topic k)\n",
    "       Outputs: Updated doc_j, m_k, k_idx '''\n",
    "    \n",
    "    k_jt = doc_j['jt_info'][1, t_idx]\n",
    "    \n",
    "    # Delete table \n",
    "    #doc_j['jt_info'] = np.delete(doc_j['jt_info'],t_idx, axis =1) # remove table, i.e. del column\n",
    "    doc_j['jt_info'][:,t_idx] = np.zeros(3)\n",
    "    m_k[k_jt] -= 1\n",
    "    \n",
    "    if m_k[k_jt] == 0: \n",
    "        k_idx.remove(k_jt) #if no more tables with topic k, remove topic\n",
    "\n",
    "    return doc_j, m_k, k_idx\n",
    "\n",
    "\n",
    "\n",
    "def fk_m_xji(n_kv, word):\n",
    "    '''Conditional density of x_ji given k and all data items except x_ji'''\n",
    "    return n_kv[word,:] / n_kv.sum(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def post_pvals_t(doc_j, k_jt, fk, m_k, alpha, gamma):\n",
    "    '''Generate posterior pvals for both selecting a new or existing table'''\n",
    "    # if t is NOT NEW\n",
    "    n_jt = doc_j['jt_info'][2,:] # get counts across tables\n",
    "    t_post = n_jt*fk[k_jt]\n",
    "\n",
    "    # if t is NEW\n",
    "    p_xji=0\n",
    "    for k in range(len(k_idx)): # compute p_xji based on paper\n",
    "        p_xji += m_k[k] * fk[k]\n",
    "\n",
    "    p_xji = p_xji + (gamma / V) \n",
    "    t_post[0] = (alpha * p_xji)/ (sum(m_k) + gamma) # t if new store as first\n",
    "    \n",
    "    return t_post\n",
    "\n",
    "\n",
    "def post_pvals_k_new_t(m_k, fk, k_idx, gamma, V):\n",
    "    '''If new table selected, generate posterior pvals for selecting a new or existing topic'''\n",
    "    kt_post = (m_k*fk)[k_idx] # existing topic\n",
    "    kt_post[0] = gamma /V # new topic\n",
    "    \n",
    "    return kt_post\n",
    "\n",
    "\n",
    "\n",
    "def new_topic(n_kv, m_k, k_idx, beta, V):\n",
    "    '''If new topic selected, get new topic k and extend structures k_idx (topic idx), n_kv (word-topic matrix), \n",
    "       m_k (tables per topic) for later updates. \n",
    "       Output: new topic and extended structures'''\n",
    "    # Create new topic\n",
    "    for idx, k in enumerate(k_idx):\n",
    "        if idx != k: \n",
    "            break\n",
    "        else:\n",
    "            new_k = len(k_idx)\n",
    "\n",
    "    # Append new topic to list of topics, add column to word-topic matrix, extend table-topic array\n",
    "    k_idx.append(new_k)\n",
    "    n_kv = np.c_[n_kv, np.ones((V, 1)) * beta]\n",
    "    m_k = np.r_[m_k, 0]\n",
    "  \n",
    "    return new_k, n_kv, m_k, k_idx\n",
    "\n",
    "\n",
    "def new_table(new_k, m_k, doc_j, word):\n",
    "    '''If new table selected, get new table idx and extend structures doc_j jt_info and n_jtw for \n",
    "       later updates\n",
    "       Output: new table and extended structures'''\n",
    "\n",
    "    for t_idx, t in enumerate(doc_j['jt_info'][0,:]):\n",
    "        if t_idx != t: \n",
    "            break\n",
    "        else:\n",
    "            new_t = doc_j['jt_info'].shape[1]\n",
    "\n",
    "    # Add column to doc's 'jt_info' array, set topic of new table,extend discretized word cnt dict\n",
    "    # to allocate word in new table\n",
    "    doc_j['jt_info'] = np.c_[doc_j['jt_info'], np.zeros((3,1), dtype=int)]\n",
    "    doc_j['jt_info'][1, new_t] = new_k\n",
    "    doc_j['n_jtw'].append({word:0})\n",
    "    \n",
    "    \n",
    "    return new_t, doc_j\n",
    "\n",
    "\n",
    "def assign_to_table(doc_j, new_t, n_kv, word):\n",
    "    '''Assign word to table new_t with topic new_k in doc_j, add counts to overall table count,  \n",
    "       word-topic matrix and discretized table word counts\n",
    "       Outputs: updated doc_j and n_kv'''\n",
    "    \n",
    "    # Get topic of table (either new or old)\n",
    "    if doc_j['jt_info'].shape[1] < new_t + 1:\n",
    "        print( new_t, doc_j)\n",
    "    new_k = doc_j['jt_info'][1, new_t]\n",
    "\n",
    "    # Seat at table, assign corresponding topic, add 1 to overall table count\n",
    "    doc_j['jt_info'][:2, new_t] = np.array([new_t , new_k])\n",
    "    doc_j['jt_info'][2, new_t] += 1\n",
    "\n",
    "    # Get word-table assignment idx, add 1 to discretized word cnt dictionary for that table\n",
    "    doc_j['w_tbl_idx'][i] = new_t\n",
    "    doc_j['n_jtw'][new_t][word] = doc_j['n_jtw'][new_t].get(word, 0) + 1\n",
    "    \n",
    "    # Add 1 for word in word-topic count matrix\n",
    "    n_kv[word, new_k] += 1\n",
    "    \n",
    "    return doc_j, n_kv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "########### SAMPLING K FUNCTIONS ##########\n",
    "##########################################\n",
    "\n",
    "def sampling_k(doc_j, tbl, n_kv, m_k, k_idx):\n",
    "    '''For each TABLE in document j (doc_j), sample for posterior distribution of k and update\n",
    "       table and topic assignments, as well as other count structures within jt_info, n_jtw, m_k, and n_kv\n",
    "       Output: updated doc_j, m_k, n_kv, and topic idx (k_idx)'''\n",
    "    \n",
    "    #### START of Sampling k loop through tables, (skip first index always, 0 = dummy idx) ####\n",
    "    if tbl != 0: \n",
    "\n",
    "        # Get topic k, remove all components from table t associated with topic k\n",
    "        doc_j, m_k, k_idx = remove_Xvec_ji(doc_j, tbl, m_k, k_idx)\n",
    "\n",
    "        # Samples posterior p-vals K\n",
    "        post_k = post_pvals_k(doc_j, tbl, n_kv, m_k, k_idx, V, beta)\n",
    "        post_k /= post_k.sum()\n",
    "        \n",
    "        # Select most likely topic for table\n",
    "        k_samp_idx = np.random.multinomial(1, post_k).argmax()\n",
    "        \n",
    "        new_k = k_idx[k_samp_idx]\n",
    "\n",
    "        ## New topic selected\n",
    "        if new_k == 0:\n",
    "\n",
    "            # Create new topic\n",
    "            new_k, n_kv, m_k, k_idx = new_topic(n_kv, m_k, k_idx, beta, V)\n",
    "        \n",
    "        # Add table to topic k count\n",
    "        m_k[new_k] += 1\n",
    "        \n",
    "        doc_j, n_kv = rearranging_k_counts(doc_j, tbl, new_k, n_kv)\n",
    "            \n",
    "\n",
    "    return doc_j, n_kv, m_k, k_idx \n",
    "\n",
    "\n",
    "\n",
    "def remove_Xvec_ji(doc_j, tbl, m_k, k_idx):\n",
    "    '''Remove table from topic k (i.e. related removing all components associated to table t later)\n",
    "       If table becomes empty, remove topic'''\n",
    "    \n",
    "    # Get topic k, remove all components from table t associated with topic k\n",
    "    k_jt = doc_j['jt_info'][1, tbl]\n",
    "    m_k[k_jt] -= 1 # remove from table-topic vector\n",
    "\n",
    "    if m_k[k_jt] == 0:\n",
    "        k_idx.remove(k_jt) # if no more tables with topic k, remove topic k and set table's topic to 0\n",
    "        doc_j['jt_info'][1, tbl] = 0\n",
    "    \n",
    "    return doc_j, m_k, k_idx\n",
    "\n",
    "\n",
    "def post_pvals_k(doc_j, tbl, n_kv, m_k, k_idx, V, beta):\n",
    "    '''Compute explicit posterior pvals distribution based on dirichlet-multinomial form'''\n",
    "    \n",
    "    # Topic k of table t\n",
    "    k_jt = doc_j['jt_info'][1, tbl]\n",
    "\n",
    "    # Remove all counts associated with topic k in table t, from overall topic counts (n_k)\n",
    "    \n",
    "    n_kv = n_kv.copy() #### NOTE: fix, remind me to never disregard what Cliburn says in class 20 times about slicing\n",
    "    n_k = n_kv.sum(axis = 0)\n",
    "    n_jt = doc_j['jt_info'][2, tbl]\n",
    "    n_k[k_jt] -= n_jt\n",
    "    n_k = n_k[k_idx]\n",
    "\n",
    "    # Initialized k posterior in log-form for simplicity, this computes f_k^{-X_ji} \n",
    "    # has Dirichlet-Multinomial form\n",
    "    log_post_k = np.log(m_k[k_idx]) + gammaln(n_k) - gammaln(n_k + n_jt)\n",
    "    log_post_k_new = np.log(gamma) + gammaln(V*beta) - gammaln((V*beta) + n_jt)\n",
    "\n",
    "    # Remove individual word counts associated with topic k\n",
    "    # add their contributions to k posterior\n",
    "    for w_key, w_cnt in doc_j['n_jtw'][tbl].items():\n",
    "\n",
    "        assert w_cnt >= 0\n",
    "        if w_cnt == 0: # if word count is 0 skip\n",
    "            continue\n",
    "\n",
    "        # For word w, get counts across topics - if zero set as beta\n",
    "        w_cnt_k = n_kv[w_key, :]\n",
    "        w_cnt_k[w_cnt_k == 0] = beta\n",
    "        \n",
    "        if np.any(w_cnt_k <= 0): print(\"pre- check\", j, tbl, k_jt, w_key, w_cnt_k, w_cnt)\n",
    "\n",
    "        # For specific topic k, remove count from associated table t\n",
    "        w_cnt_k[k_jt] -= w_cnt\n",
    "        w_cnt_k = w_cnt_k[k_idx]\n",
    "\n",
    "        w_cnt_k[0] = 1\n",
    "        if np.any(w_cnt_k <= 0): print(\"check\", j, tbl, k_jt, w_key, w_cnt_k, w_cnt)\n",
    "\n",
    "        # Add contributions\n",
    "        log_post_k += gammaln(w_cnt_k  + w_cnt) - gammaln(w_cnt_k)\n",
    "        log_post_k_new += gammaln(beta + w_cnt) - gammaln(beta)\n",
    "\n",
    "\n",
    "    # set K new\n",
    "    log_post_k[0] = log_post_k_new\n",
    "\n",
    "    # Bring back to non-log realm, normalize k-posterior \n",
    "    post_k = np.exp(log_post_k - log_post_k.max())\n",
    "\n",
    "    return post_k\n",
    "\n",
    "\n",
    "def rearranging_k_counts(doc_j, tbl, new_k, n_kv):\n",
    "    '''For sampled k, rearrange counts for topics accordingly (i.e. if a new k was selected, subtract\n",
    "       from previous k and add to new k in word-topic matrix)'''\n",
    "    # If new topic for table t is selected, set topic to new topic\n",
    "    k_jt = doc_j['jt_info'][1, tbl]\n",
    "    if new_k != k_jt: \n",
    "        doc_j['jt_info'][1, tbl] = new_k\n",
    "\n",
    "        # On word-topic matrix, move counts from old topic to new topic\n",
    "        for k, cnt in doc_j['n_jtw'][tbl].items():\n",
    "            if k_jt != 0: \n",
    "                n_kv[k, k_jt] -= cnt\n",
    "\n",
    "            n_kv[k, new_k] += cnt\n",
    "            \n",
    "    return doc_j, n_kv\n",
    "            \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'doc_j' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d739035833d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdoc_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-683fea51e845>\u001b[0m in \u001b[0;36msampling_t\u001b[0;34m(doc_j, i, word, n_kv, m_k, k_idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mk_jt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdoc_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_x_ji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_jt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#### Sampling t ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-683fea51e845>\u001b[0m in \u001b[0;36mremove_x_ji\u001b[0;34m(t_idx, k_jt, word)\u001b[0m\n\u001b[1;32m     65\u001b[0m                  updates on doc_j, m_k (tables in topic k), and k_idx (topics) from remove_table fcn '''\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mdoc_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_jtw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# remove from dictionary table cnt n_jtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mdoc_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jt_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# remove from table cnt n_jt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mn_kv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_jt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# remove from topic count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'doc_j' referenced before assignment"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:272: RuntimeWarning: divide by zero encountered in log\n"
>>>>>>> 63ec2fabad731593789f6233ccb385fce45391db
     ]
    }
   ],
   "source": [
    "############################################\n",
    "######## Hierarchical Dirichlet ############\n",
    "############################################\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "for j, x_i in enumerate(x_ji):\n",
    "    \n",
    "    \n",
    "    doc_j = docs_dict[j]\n",
    "                \n",
    "    for i, w in enumerate(x_i):\n",
    "        \n",
    "     \n",
    "        doc_j, n_kv, m_k, k_idx = sampling_t(doc_j, i, w, n_kv, m_k, k_idx)\n",
    "        \n",
    "        \n",
    "    docs_dict[j] = doc_j\n",
    "\n",
    "\n",
    "    \n",
    "for j in range(D):\n",
    "    doc_j = docs_dict[j]\n",
    "    for tbl in doc_j['jt_info'][0, :]:\n",
    "        \n",
    "        doc_j, n_kv, m_k, k_idx = sampling_k(doc_j, tbl, n_kv, m_k, k_idx)\n",
    "    \n",
    "    docs_dict[j] = doc_j\n",
    "    \n"
=======
    "for z in range(5):\n",
    "    for j, x_i in enumerate(x_ji):\n",
    "\n",
    "\n",
    "        doc_j = docs_dict[j]\n",
    "\n",
    "        for i, w in enumerate(x_i):\n",
    "\n",
    "            doc_j, n_kv, m_k, k_idx = sampling_t(doc_j, i, w, n_kv, m_k, k_idx)\n",
    "\n",
    "        docs_dict[j] = doc_j\n",
    "\n",
    "    for j in range(D):\n",
    "        doc_j = docs_dict[j]\n",
    "        for tbl in doc_j['jt_info'][0, :]:\n",
    "\n",
    "            doc_j, n_kv, m_k, k_idx = sampling_k(doc_j, tbl, n_kv, m_k, k_idx)\n",
    "\n",
    "        docs_dict[j] = doc_j\n",
    "\n",
    "    \n",
    "\n",
    "\n"
>>>>>>> 63ec2fabad731593789f6233ccb385fce45391db
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jt_info': array([[ 0,  0,  0,  3,  0,  0,  6,  7],\n",
       "        [ 0,  0,  0,  1,  0,  0,  1,  1],\n",
       "        [ 0,  0,  0, 20,  0,  0,  3,  3]]),\n",
       " 'w_tbl_idx': array([3, 6, 3, 3, 7, 3, 3, 7, 3, 3, 3, 3, 3, 7, 3, 6, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 6, 3]),\n",
       " 'n_jtw': [{0.5: 0.5},\n",
       "  {6: 0, 45: 0, 47: 0, 53: 0, 62: 0, 40: 0, 41: 0, 55: 0, 56: 0, 61: 0},\n",
       "  {40: 0,\n",
       "   41: 0,\n",
       "   42: 0,\n",
       "   43: 0,\n",
       "   44: 0,\n",
       "   46: 0,\n",
       "   49: 0,\n",
       "   51: 0,\n",
       "   55: 0,\n",
       "   56: 0,\n",
       "   57: 0,\n",
       "   59: 0,\n",
       "   60: 0,\n",
       "   64: 0,\n",
       "   6: 0,\n",
       "   47: 0,\n",
       "   52: 0,\n",
       "   58: 0,\n",
       "   63: 0},\n",
       "  {48: 1,\n",
       "   50: 1,\n",
       "   54: 0,\n",
       "   58: 1,\n",
       "   61: 1,\n",
       "   63: 0,\n",
       "   45: 1,\n",
       "   51: 1,\n",
       "   53: 1,\n",
       "   62: 1,\n",
       "   42: 1,\n",
       "   46: 0,\n",
       "   49: 1,\n",
       "   57: 1,\n",
       "   60: 1,\n",
       "   64: 1,\n",
       "   6: 1,\n",
       "   40: 0,\n",
       "   44: 1,\n",
       "   47: 1,\n",
       "   52: 0,\n",
       "   55: 1,\n",
       "   41: 1,\n",
       "   56: 1,\n",
       "   59: 1},\n",
       "  {52: 0},\n",
       "  {46: 0},\n",
       "  {48: 0,\n",
       "   49: 0,\n",
       "   50: 0,\n",
       "   54: 1,\n",
       "   60: 0,\n",
       "   6: 0,\n",
       "   45: 0,\n",
       "   51: 0,\n",
       "   61: 0,\n",
       "   62: 0,\n",
       "   57: 0,\n",
       "   64: 0,\n",
       "   40: 1,\n",
       "   63: 1},\n",
       "  {41: 0, 56: 0, 43: 1, 46: 1, 52: 1}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Example of table indexes left behind  in 'jt_info' array (i.e. 0 but not as the first entry)\n",
    "docs_dict[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
