{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import data_preproc\n",
    "from data_preproc import data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca, docs = data_preproc(\"tm_test_data.csv\") # load vocab and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special class \n",
    "class DefaultDict(dict):\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "        dict.__init__(self)\n",
    "    def __getitem__(self, k):\n",
    "        return dict.__getitem__(self, k) if k in self else self.v\n",
    "    def update(self, d):\n",
    "        dict.update(self, d)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing default values for start of alg\n",
    "\n",
    "# Hyperparameters (concentration parms of DP distributions)\n",
    "gamma = np.random.gamma(1, 1)\n",
    "alpha = np.random.gamma(1, 1)\n",
    "beta = .5\n",
    "\n",
    "# size of vocabulary \n",
    "V = len(voca)\n",
    "# To see words type voca.vocas\n",
    "\n",
    "# Number of documents \n",
    "M = len(docs)\n",
    "\n",
    "# Table index for document j\n",
    "using_t = [[0] for j in range(M)]\n",
    "\n",
    "# Dish index - 0 means draw a new topic \n",
    "k = 0\n",
    "using_k = [0]\n",
    "\n",
    "\n",
    "# x is data, t is table index, k is topic index, n is number of terms, m is number of tables\n",
    "\n",
    "# Vocabulary for each doc-term - this is the input data and doesn't change \n",
    "x_ji = docs\n",
    "\n",
    "# Topics of document and table\n",
    "k_jt = [np.zeros(1 ,dtype=int) for j in range(M)]\n",
    "\n",
    "# Number of terms for each table of document\n",
    "n_jt = [np.zeros(1 ,dtype=int) for j in range(M)]   \n",
    "\n",
    "# Number of terms for each table and vocabulary of document \n",
    "n_jtv = [[None] for j in range(M)]\n",
    "\n",
    "\n",
    "m = 0\n",
    "# Number of tables for each topic\n",
    "m_k = np.ones(1 ,dtype=int)  \n",
    "\n",
    "# Number of terms for each topic ( + beta * V )\n",
    "n_k = np.array([beta * V]) \n",
    "\n",
    "# Number of terms for each topic and vocabulary ( + beta )\n",
    "n_kv = [DefaultDict(0)]            \n",
    "\n",
    "# Table for each document and term (-1 means not-assigned)\n",
    "t_ji = [np.zeros(len(x_i), dtype=int) - 1 for x_i in docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers ## \n",
    "\n",
    "# Function that takes v (term index) and returns a list that represents the distribution of a term across topics -- i.e. each element is the proportion of terms in topic k that are term v \n",
    "def calc_f_k(v):\n",
    "    return [n_kv[v] for n_kv in n_kv]/n_k\n",
    "\n",
    "\n",
    "# Function that calculates the posterior distribution of tables for doc j / arguments: j - doc index,f_k - distribution of term across topics \n",
    "\n",
    "def calc_table_posterior(j, f_k, using_t, n_jt):\n",
    "    \n",
    "    # Store list of tables for doc j as using_t\n",
    "    using_t = using_t[j]\n",
    "    \n",
    "    # Number of terms in doc j at each table times disibutrion of terms across topics ## CHECK THIS \n",
    "    p_t = n_jt[j][using_t] * f_k[k_jt[j][using_t]]\n",
    "    \n",
    "    # Sum of number of tables across topics weighted by f_k + gamma/(vocab size) -- this corresponds with the probability of selecting a new table \n",
    "    p_x_ji = np.inner(m_k, f_k) + gamma / V\n",
    "    \n",
    "    # Storing probability of new table as first element \n",
    "    p_t[0] = p_x_ji * alpha / (gamma + m)\n",
    "\n",
    "    # Return likelihood over prior \n",
    "    return p_t / p_t.sum()\n",
    "\n",
    "\n",
    "def calc_dish_posterior_w(f_k):\n",
    "    \"calculate dish(topic) posterior when one word is removed\"\n",
    "    \n",
    "    p_k = (m_k * f_k)[using_k]\n",
    "    p_k[0] = gamma / V\n",
    "    \n",
    "    return p_k / p_k.sum()\n",
    "    \n",
    "    \n",
    "def calc_dish_posterior_t(j, t, n_k, n_jt, n_jtv):\n",
    "    \"calculate dish(topic) posterior when one table is removed\"\n",
    "    k_old = k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "    \n",
    "    Vbeta = V * beta\n",
    "    n_k = n_k.copy()\n",
    "    n_jt2 = n_jt.copy()[j][t]\n",
    "    n_k[k_old] -= n_jt2\n",
    "    n_k = n_k[using_k]\n",
    "    log_p_k = np.log(m_k[using_k]) + gammaln(n_k) - gammaln(n_k + n_jt2)\n",
    "    log_p_k_new = np.log(gamma) + gammaln(Vbeta) - gammaln(Vbeta + n_jt2)\n",
    "\n",
    "    gammaln_beta = gammaln(beta)\n",
    "    for w, n_jtw in n_jtv[j][t].items():\n",
    "        assert n_jtw >= 0\n",
    "        if n_jtw == 0: continue\n",
    "        n_kw = np.array([n.get(w, beta) for n in n_kv])\n",
    "        n_kw[k_old] -= n_jtw\n",
    "        n_kw = n_kw[using_k]\n",
    "        n_kw[0] = 1 # dummy for logarithm's warning\n",
    "        if np.any(n_kw <= 0): print(n_kw) # for debug\n",
    "        log_p_k += gammaln(n_kw + n_jtw) - gammaln(n_kw)\n",
    "        log_p_k_new += gammaln(beta + n_jtw) - gammaln_beta\n",
    "        \n",
    "        \n",
    "    log_p_k[0] = log_p_k_new\n",
    "    \n",
    "    p_k = np.exp(log_p_k - log_p_k.max())\n",
    "    return p_k / p_k.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPLDA Alg ### \n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "# g = epochs\n",
    "for g in range(5):\n",
    "    \n",
    "# Loop - sampling_t - j is doc index (e.g. first doc is 0), i is term index (0 is first element of global vocabulary voca.vocas)\n",
    "\n",
    "    # Loop through the data \n",
    "    for j, x_i in enumerate(x_ji):\n",
    "        \n",
    "        # For each doc, loop through each term\n",
    "        for i in range(len(x_i)):\n",
    "            \n",
    "            ### Reassign table for term i in document j ###\n",
    "            t = t_ji[j][i]\n",
    "            if t  > 0:\n",
    "                k = k_jt[j][t]\n",
    "                assert k > 0\n",
    "        \n",
    "                # decrease counters\n",
    "                v = x_ji[j][i]\n",
    "                n_kv[k][v] -= 1\n",
    "                n_k[k] -= 1\n",
    "                n_jt[j][t] -= 1\n",
    "                n_jtv[j][t][v] -= 1\n",
    "        \n",
    "                if n_jt[j][t] == 0:\n",
    "                    \n",
    "                    # Remove table \n",
    "                    \n",
    "                    # Set topic index at doc j and table t to k\n",
    "                    k = k_jt[j][t]\n",
    "                    \n",
    "                    # Remove t from list of tables being used in doc j\n",
    "                    using_t[j].remove(t)\n",
    "                    \n",
    "                    # Decrease number of tables for topic k by 1\n",
    "                    m_k[k] -= 1\n",
    "                    # Decrease number of tables overall (?) by 1\n",
    "                    m -= 1\n",
    "                    assert m_k[k] >= 0\n",
    "                    \n",
    "                    # If number of tables for topic k is 0 remove topic\n",
    "                    if m_k[k] == 0:\n",
    "                        using_k.remove(k)\n",
    "        \n",
    "                                    \n",
    "            # Store term index as v\n",
    "            v = x_ji[j][i]\n",
    "            \n",
    "            # Calculate the distribution of v across the topics -- f_k will be the base distribution for the calc_table_posterior function \n",
    "            f_k = calc_f_k(v)\n",
    "            assert f_k[0] == 0 # f_k[0] is a dummy and will be erased\n",
    "        \n",
    "            \n",
    "            # Calculating the posterior distribution of tables --  p(t_ji=t)\n",
    "            p_t = calc_table_posterior(j, f_k, using_t, n_jt)\n",
    "            \n",
    "            # This just prints some results while the alg runs - blocking out for now     \n",
    "            # if len(p_t) > 1 and p_t[1] < 0: dump()\n",
    "                \n",
    "            # Sample from the posterior and assigned the corresponding table index to t_new (not necessarily a new table - it's a new sample)\n",
    "            t_new = using_t[j][np.random.multinomial(1, p_t).argmax()]\n",
    "            \n",
    "            # If t_new == 0 (i.e. the table is new)\n",
    "            if t_new == 0:\n",
    "                \n",
    "                # Calculate the posterior distribution of topics \n",
    "                p_k = calc_dish_posterior_w(f_k)\n",
    "                \n",
    "                # Sample from this posterior distribution and assign the corresponding topic index to k_new \n",
    "                k_new = using_k[np.random.multinomial(1, p_k).argmax()]\n",
    "                \n",
    "                # If k_new == 0 (i.e. the topic is new)\n",
    "                if k_new == 0:\n",
    "                    \n",
    "                    # Add new dish and store as k_new \n",
    "                    for k_new, k in enumerate(using_k):\n",
    "                        if k_new != k: break\n",
    "                    else:\n",
    "                        k_new = len(using_k)\n",
    "                        if k_new >= len(n_kv):\n",
    "                            n_k = np.resize(n_k, k_new + 1)\n",
    "                            m_k = np.resize(m_k, k_new + 1)\n",
    "                            n_kv.append(None)\n",
    "                        assert k_new == using_k[-1] + 1\n",
    "                        assert k_new < len(n_kv)\n",
    "    \n",
    "                    using_k.insert(k_new, k_new)\n",
    "                    n_k[k_new] = beta * V\n",
    "                    m_k[k_new] = 0\n",
    "                    n_kv[k_new] = DefaultDict(beta)\n",
    "                    \n",
    "                assert k_new in using_k\n",
    "                \n",
    "                for t_new, t in enumerate(using_t[j]):\n",
    "                    if t_new != t: break\n",
    "                else:\n",
    "                    t_new = len(using_t[j])\n",
    "                    n_jt[j].resize(t_new+1)\n",
    "                    k_jt[j].resize(t_new+1)\n",
    "                    n_jtv[j].append(None)\n",
    "            \n",
    "                using_t[j].insert(t_new, t_new)\n",
    "                n_jt[j][t_new] = 0  # to make sure\n",
    "                n_jtv[j][t_new] = DefaultDict(0)\n",
    "            \n",
    "                k_jt[j][t_new] = k_new\n",
    "                \n",
    "                m_k[k_new] += 1\n",
    "                \n",
    "                m += 1\n",
    "            \n",
    "            assert t_new in using_t[j]\n",
    "            t_ji[j][i] = t_new\n",
    "            n_jt[j][t_new] += 1\n",
    "    \n",
    "            k_new = k_jt[j][t_new]\n",
    "            n_k[k_new] += 1\n",
    "    \n",
    "            v = x_ji[j][i]\n",
    "            n_kv[k_new][v] += 1\n",
    "            n_jtv[j][t_new][v] += 1\n",
    "            \n",
    "                \n",
    "    for j in range(M):\n",
    "        for t in using_t[j]:\n",
    "            if t != 0: \n",
    "                \"\"\"sampling k (dish=topic) from posterior\"\"\"\n",
    "    \n",
    "                #This makes the table leave from its dish and only the table counter decrease. The word counters (n_k and n_kv) stay.\n",
    "                \n",
    "                k = k_jt[j][t]\n",
    "                assert k > 0\n",
    "                assert m_k[k] > 0\n",
    "                \n",
    "                m_k[k] -= 1\n",
    "                m -= 1\n",
    "                if m_k[k] == 0:\n",
    "                    using_k.remove(k)\n",
    "                    k_jt[j][t] = 0\n",
    "                #\n",
    "                    \n",
    "                # sampling of k\n",
    "                p_k = calc_dish_posterior_t(j, t, n_k, n_jt, n_jtv)\n",
    "                \n",
    "                k_new = using_k[np.random.multinomial(1, p_k).argmax()]\n",
    "                \n",
    "\n",
    "                if k_new == 0:\n",
    "                    # Add new dish  \n",
    "                    for k_new, k in enumerate(using_k):\n",
    "                        if k_new != k: break\n",
    "                    else:\n",
    "                        k_new = len(using_k)\n",
    "                        if k_new >= len(n_kv):\n",
    "                            n_k = np.resize(n_k, k_new + 1)\n",
    "                            m_k = np.resize(m_k, k_new + 1)\n",
    "                            n_kv.append(None)\n",
    "                        assert k_new == using_k[-1] + 1\n",
    "                        assert k_new < len(n_kv)\n",
    "                \n",
    "                    using_k.insert(k_new, k_new)\n",
    "                    n_k[k_new] = beta * V\n",
    "                    m_k[k_new] = 0\n",
    "                    n_kv[k_new] = DefaultDict(beta)\n",
    "                    \n",
    "      \n",
    "                    \n",
    "                m += 1\n",
    "                m_k[k_new] += 1\n",
    "            \n",
    "                k_old = k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "                if k_new != k_old:\n",
    "                    k_jt[j][t] = k_new\n",
    "            \n",
    "                    n_jt2 = n_jt.copy()[j][t]\n",
    "                    if k_old != 0: n_k[k_old] -= n_jt2\n",
    "                    n_k[k_new] += n_jt2\n",
    "                    for v, n in n_jtv[j][t].items():\n",
    "                        if k_old != 0: n_kv[k_old][v] -= n\n",
    "                        n_kv[k_new][v] += n\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 5.5, 1.5, 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([n.get(386, beta) for n in n_kv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1351: 0.5,\n",
       " 376: 0.5,\n",
       " 396: 0.5,\n",
       " 620: 0.5,\n",
       " 1035: 0.5,\n",
       " 1084: 0.5,\n",
       " 1122: 0.5,\n",
       " 1176: 0.5,\n",
       " 1225: 0.5,\n",
       " 312: 0.5,\n",
       " 584: 0.5,\n",
       " 630: 0.5,\n",
       " 1512: 0.5,\n",
       " 88: 0.5,\n",
       " 294: 0.5,\n",
       " 330: 0.5,\n",
       " 402: 0.5,\n",
       " 507: 0.5,\n",
       " 680: 0.5,\n",
       " 688: 0.5,\n",
       " 809: 0.5,\n",
       " 921: 0.5,\n",
       " 1008: 0.5,\n",
       " 1253: 0.5,\n",
       " 1427: 0.5,\n",
       " 1441: 0.5,\n",
       " 373: 0.5,\n",
       " 386: 1.5,\n",
       " 852: 0.5,\n",
       " 996: 0.5,\n",
       " 1053: 0.5,\n",
       " 1467: 1.5,\n",
       " 618: 0.5,\n",
       " 1006: 0.5,\n",
       " 80: 0.5,\n",
       " 657: 0.5,\n",
       " 1221: 0.5,\n",
       " 114: 0.5,\n",
       " 1218: 0.5,\n",
       " 697: 0.5,\n",
       " 682: 0.5,\n",
       " 420: 0.5,\n",
       " 458: 0.5,\n",
       " 738: 0.5,\n",
       " 1406: 0.5,\n",
       " 39: 0.5,\n",
       " 52: 1.5,\n",
       " 79: 0.5,\n",
       " 271: 0.5,\n",
       " 687: 1.5,\n",
       " 1196: 0.5,\n",
       " 1208: 1.5,\n",
       " 1392: 0.5,\n",
       " 41: 0.5,\n",
       " 77: 0.5,\n",
       " 111: 0.5,\n",
       " 404: 0.5,\n",
       " 492: 0.5,\n",
       " 1180: 1.5,\n",
       " 1363: 0.5,\n",
       " 86: 0.5,\n",
       " 1068: 0.5,\n",
       " 385: 1.5,\n",
       " 664: 0.5,\n",
       " 1281: 0.5,\n",
       " 1498: 0.5,\n",
       " 56: 1.5,\n",
       " 1460: 1.5,\n",
       " 993: 1.5,\n",
       " 1044: 1.5,\n",
       " 1276: 1.5,\n",
       " 1282: 0.5,\n",
       " 1405: 2.5,\n",
       " 1455: 0.5,\n",
       " 734: 0.5,\n",
       " 529: 0.5,\n",
       " 834: 0.5,\n",
       " 994: 0.5,\n",
       " 1549: 0.5,\n",
       " 160: 0.5,\n",
       " 1475: 0.5,\n",
       " 1033: 0.5,\n",
       " 198: 0.5,\n",
       " 915: 0.5,\n",
       " 974: 0.5,\n",
       " 1338: 0.5,\n",
       " 154: 0.5,\n",
       " 161: 0.5,\n",
       " 299: 0.5,\n",
       " 796: 0.5,\n",
       " 1187: 0.5,\n",
       " 96: 0.5,\n",
       " 906: 0.5,\n",
       " 941: 0.5,\n",
       " 1194: 0.5,\n",
       " 1236: 0.5,\n",
       " 61: 0.5,\n",
       " 845: 0.5,\n",
       " 893: 0.5,\n",
       " 214: 0.5,\n",
       " 251: 0.5,\n",
       " 1542: 0.5,\n",
       " 679: 0.5,\n",
       " 481: 0.5,\n",
       " 540: 0.5,\n",
       " 1083: 0.5,\n",
       " 1027: 1.5,\n",
       " 1290: 0.5,\n",
       " 1309: 0.5,\n",
       " 65: 0.5,\n",
       " 597: 0.5,\n",
       " 665: 0.5,\n",
       " 960: 0.5,\n",
       " 119: 0.5,\n",
       " 438: 0.5,\n",
       " 761: 1.5,\n",
       " 779: 1.5,\n",
       " 832: 0.5,\n",
       " 1267: 0.5,\n",
       " 1295: 0.5,\n",
       " 1501: 0.5,\n",
       " 46: 0.5,\n",
       " 280: 1.5,\n",
       " 304: 0.5,\n",
       " 961: 0.5,\n",
       " 1138: 1.5,\n",
       " 1161: 0.5,\n",
       " 1261: 0.5,\n",
       " 153: 1.5,\n",
       " 917: 0.5,\n",
       " 1082: 0.5,\n",
       " 1157: 0.5,\n",
       " 249: 0.5,\n",
       " 1224: 0.5,\n",
       " 369: 1.5,\n",
       " 778: 1.5,\n",
       " 1409: 0.5,\n",
       " 744: 1.5,\n",
       " 128: 1.5,\n",
       " 424: 1.5,\n",
       " 946: 1.5,\n",
       " 964: 1.5,\n",
       " 1233: 1.5,\n",
       " 1271: 1.5,\n",
       " 137: 0.5,\n",
       " 303: 0.5,\n",
       " 685: 0.5,\n",
       " 1026: 0.5,\n",
       " 1056: 0.5,\n",
       " 1497: 0.5,\n",
       " 110: 0.5,\n",
       " 1211: 0.5,\n",
       " 1251: 0.5,\n",
       " 957: 0.5,\n",
       " 1284: 0.5,\n",
       " 334: 1.5,\n",
       " 830: 1.5,\n",
       " 7: 1.5,\n",
       " 1509: 0.5,\n",
       " 572: 1.5,\n",
       " 575: 2.5,\n",
       " 751: 1.5,\n",
       " 1355: 1.5,\n",
       " 1428: 1.5,\n",
       " 54: 0.5,\n",
       " 737: 0.5,\n",
       " 819: 0.5,\n",
       " 950: 0.5,\n",
       " 1277: 0.5,\n",
       " 1382: 0.5,\n",
       " 1434: 1.5,\n",
       " 1534: 0.5,\n",
       " 1538: 0.5,\n",
       " 36: 0.5,\n",
       " 53: 0.5,\n",
       " 125: 0.5,\n",
       " 553: 0.5,\n",
       " 768: 1.5,\n",
       " 785: 0.5,\n",
       " 999: 0.5,\n",
       " 1051: 0.5,\n",
       " 1299: 0.5,\n",
       " 72: 1.5,\n",
       " 136: 1.5,\n",
       " 352: 0.5,\n",
       " 399: 1.5,\n",
       " 459: 0.5,\n",
       " 592: 0.5,\n",
       " 637: 0.5,\n",
       " 666: 1.5,\n",
       " 782: 0.5,\n",
       " 842: 0.5,\n",
       " 997: 0.5,\n",
       " 1126: 0.5,\n",
       " 1204: 1.5,\n",
       " 1296: 1.5,\n",
       " 1314: 0.5,\n",
       " 1449: 0.5,\n",
       " 1477: 0.5,\n",
       " 241: 1.5,\n",
       " 987: 0.5,\n",
       " 1052: 0.5,\n",
       " 1330: 0.5,\n",
       " 564: 0.5,\n",
       " 591: 0.5,\n",
       " 485: 0.5,\n",
       " 766: 1.5,\n",
       " 1173: 0.5,\n",
       " 951: 1.5,\n",
       " 1374: 0.5,\n",
       " 952: 0.5,\n",
       " 1183: 0.5,\n",
       " 281: 1.5,\n",
       " 1073: 1.5,\n",
       " 1024: 1.5}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_kv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = DefaultDict(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing[3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity Checks ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sum of the words among all the tables in doc 0 equals the sum of the words in doc 0\n",
    "sum(n_jt[0]) == len(x_ji[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 17, 2, 15, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_jtv[0] (0 indexes first doc) is a list of dictionaries with each dictionary item giving the vocab index at a given table -- dict.keys() gives vocab index and dict.values() gives count \n",
    "# of that term at the table \n",
    "\n",
    "# This is the sum of words at the tables of doc 0.  It should be the same as n_jt[0][1:]\n",
    "[j for j in [sum(i.values()) for i in n_jtv[0] if i is not None] if j != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for j in [sum(i.values()) for i in n_jtv[0] if i is not None] if j != 0] == [h for h in n_jt[0] if h != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the total number of tables (m_k is the distribution of tables across the 26 topics)\n",
    "sum(m_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3782"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is also the total number of tables.  n_jt is the number of words at each table of each document.  This should be the same as sum(m_k) but it's off by 1 for some reason\n",
    "sum([len(g) for g in [[j for j in i if j!=0] for i in n_jt]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_k)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00313233, 0.00035088, 0.00046339, 0.00278373,\n",
       "       0.00058005, 0.00068681, 0.00253293])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n_kv[5] for n_kv in n_kv] / n_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
