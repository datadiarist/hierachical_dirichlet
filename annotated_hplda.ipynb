{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import data_preproc\n",
    "from data_preproc import data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca, docs = data_preproc(\"tm_test_data.csv\") # load vocab and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special class \n",
    "class DefaultDict(dict):\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "        dict.__init__(self)\n",
    "    def __getitem__(self, k):\n",
    "        return dict.__getitem__(self, k) if k in self else self.v\n",
    "    def update(self, d):\n",
    "        dict.update(self, d)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing default values for start of alg\n",
    "\n",
    "# Hyperparameters (concentration parms of DP distributions)\n",
    "gamma = np.random.gamma(1, 1)\n",
    "alpha = np.random.gamma(1, 1)\n",
    "beta = .5\n",
    "\n",
    "# size of vocabulary \n",
    "V = len(voca)\n",
    "# To see words type voca.vocas\n",
    "\n",
    "# Number of documents \n",
    "M = len(docs)\n",
    "\n",
    "# Table index for document j\n",
    "using_t = [[0] for j in range(M)]\n",
    "\n",
    "# Dish index - 0 means draw a new topic \n",
    "k = 0\n",
    "using_k = [0]\n",
    "\n",
    "\n",
    "# x is data, t is table index, k is topic index, n is number of terms, m is number of tables\n",
    "\n",
    "# Vocabulary for each doc-term - this is the input data and doesn't change \n",
    "x_ji = docs\n",
    "\n",
    "# Topics of document and table\n",
    "k_jt = [np.zeros(1 ,dtype=int) for j in range(M)]\n",
    "\n",
    "# Number of terms for each table of document\n",
    "n_jt = [np.zeros(1 ,dtype=int) for j in range(M)]   \n",
    "\n",
    "# Number of terms for each table and vocabulary of document \n",
    "n_jtv = [[None] for j in range(M)]\n",
    "\n",
    "\n",
    "m = 0\n",
    "# Number of tables for each topic\n",
    "m_k = np.ones(1 ,dtype=int)  \n",
    "\n",
    "# Number of terms for each topic ( + beta * V )\n",
    "n_k = np.array([beta * V]) \n",
    "\n",
    "# Number of terms for each topic and vocabulary ( + beta )\n",
    "n_kv = [DefaultDict(0)]            \n",
    "\n",
    "# Table for each document and term (-1 means not-assigned)\n",
    "t_ji = [np.zeros(len(x_i), dtype=int) - 1 for x_i in docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers ## \n",
    "\n",
    "# Function that takes v (term index) and returns a list that represents the distribution of a term across topics -- i.e. each element is the proportion of terms in topic k that are term v \n",
    "def calc_f_k(v):\n",
    "    return [n_kv[v] for n_kv in n_kv]/n_k\n",
    "\n",
    "\n",
    "# Function that calculates the posterior distribution of tables for doc j / arguments: j - doc index,f_k - distribution of term across topics \n",
    "\n",
    "def calc_table_posterior(j, f_k, using_t, n_jt):\n",
    "    \n",
    "    # Store list of tables for doc j as using_t\n",
    "    using_t = using_t[j]\n",
    "    \n",
    "    # Number of terms in doc j at each table times disibutrion of terms across topics ## CHECK THIS \n",
    "    p_t = n_jt[j][using_t] * f_k[k_jt[j][using_t]]\n",
    "    \n",
    "    # Sum of number of tables across topics weighted by f_k + gamma/(vocab size) -- this corresponds with the probability of selecting a new table \n",
    "    p_x_ji = np.inner(m_k, f_k) + gamma / V\n",
    "    \n",
    "    # Storing probability of new table as first element \n",
    "    p_t[0] = p_x_ji * alpha / (gamma + m)\n",
    "\n",
    "    # Return likelihood over prior \n",
    "    return p_t / p_t.sum()\n",
    "\n",
    "\n",
    "def calc_dish_posterior_w(f_k):\n",
    "    \"calculate dish(topic) posterior when one word is removed\"\n",
    "    \n",
    "    p_k = (m_k * f_k)[using_k]\n",
    "    p_k[0] = gamma / V\n",
    "    \n",
    "    return p_k / p_k.sum()\n",
    "    \n",
    "    \n",
    "def calc_dish_posterior_t(j, t, n_k, n_jt, n_jtv):\n",
    "    \"calculate dish(topic) posterior when one table is removed\"\n",
    "    k_old = k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "    \n",
    "    Vbeta = V * beta\n",
    "    n_k = n_k.copy()\n",
    "    n_jt2 = n_jt.copy()[j][t]\n",
    "    n_k[k_old] -= n_jt2\n",
    "    n_k = n_k[using_k]\n",
    "    log_p_k = np.log(m_k[using_k]) + gammaln(n_k) - gammaln(n_k + n_jt2)\n",
    "    log_p_k_new = np.log(gamma) + gammaln(Vbeta) - gammaln(Vbeta + n_jt2)\n",
    "\n",
    "    gammaln_beta = gammaln(beta)\n",
    "    for w, n_jtw in n_jtv[j][t].items():\n",
    "        assert n_jtw >= 0\n",
    "        if n_jtw == 0: continue\n",
    "        n_kw = np.array([n.get(w, beta) for n in n_kv])\n",
    "        n_kw[k_old] -= n_jtw\n",
    "        n_kw = n_kw[using_k]\n",
    "        n_kw[0] = 1 # dummy for logarithm's warning\n",
    "        if np.any(n_kw <= 0): print(n_kw) # for debug\n",
    "        log_p_k += gammaln(n_kw + n_jtw) - gammaln(n_kw)\n",
    "        log_p_k_new += gammaln(beta + n_jtw) - gammaln_beta\n",
    "        \n",
    "        \n",
    "    log_p_k[0] = log_p_k_new\n",
    "    \n",
    "    p_k = np.exp(log_p_k - log_p_k.max())\n",
    "    return p_k / p_k.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPLDA Alg ### \n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "# g = epochs\n",
    "for g in range(15):\n",
    "    \n",
    "# Loop - sampling_t - j is doc index (e.g. first doc is 0), i is term index (0 is first element of global vocabulary voca.vocas)\n",
    "\n",
    "    # Loop through the data \n",
    "    for j, x_i in enumerate(x_ji):\n",
    "        \n",
    "        # For each doc, loop through each term\n",
    "        for i in range(len(x_i)):\n",
    "            \n",
    "            ### Reassign table for term i in document j ###\n",
    "            t = t_ji[j][i]\n",
    "            if t  > 0:\n",
    "                k = k_jt[j][t]\n",
    "                assert k > 0\n",
    "        \n",
    "                # decrease counters\n",
    "                v = x_ji[j][i]\n",
    "                n_kv[k][v] -= 1\n",
    "                n_k[k] -= 1\n",
    "                n_jt[j][t] -= 1\n",
    "                n_jtv[j][t][v] -= 1\n",
    "        \n",
    "                if n_jt[j][t] == 0:\n",
    "                    \n",
    "                    # Remove table \n",
    "                    \n",
    "                    # Set topic index at doc j and table t to k\n",
    "                    k = k_jt[j][t]\n",
    "                    \n",
    "                    # Remove t from list of tables being used in doc j\n",
    "                    using_t[j].remove(t)\n",
    "                    \n",
    "                    # Decrease number of tables for topic k by 1\n",
    "                    m_k[k] -= 1\n",
    "                    # Decrease number of tables overall (?) by 1\n",
    "                    m -= 1\n",
    "                    assert m_k[k] >= 0\n",
    "                    \n",
    "                    # If number of tables for topic k is 0 remove topic\n",
    "                    if m_k[k] == 0:\n",
    "                        using_k.remove(k)\n",
    "        \n",
    "                                    \n",
    "            # Store term index as v\n",
    "            v = x_ji[j][i]\n",
    "            \n",
    "            # Calculate the distribution of v across the topics -- f_k will be the base distribution for the calc_table_posterior function \n",
    "            f_k = calc_f_k(v)\n",
    "            assert f_k[0] == 0 # f_k[0] is a dummy and will be erased\n",
    "        \n",
    "            \n",
    "            # Calculating the posterior distribution of tables --  p(t_ji=t)\n",
    "            p_t = calc_table_posterior(j, f_k, using_t, n_jt)\n",
    "            \n",
    "            # This just prints some results while the alg runs - blocking out for now     \n",
    "            # if len(p_t) > 1 and p_t[1] < 0: dump()\n",
    "                \n",
    "            # Sample from the posterior and assigned the corresponding table index to t_new (not necessarily a new table - it's a new sample)\n",
    "            t_new = using_t[j][np.random.multinomial(1, p_t).argmax()]\n",
    "            \n",
    "            # If t_new == 0 (i.e. the table is new)\n",
    "            if t_new == 0:\n",
    "                \n",
    "                # Calculate the posterior distribution of topics \n",
    "                p_k = calc_dish_posterior_w(f_k)\n",
    "                \n",
    "                # Sample from this posterior distribution and assign the corresponding topic index to k_new \n",
    "                k_new = using_k[np.random.multinomial(1, p_k).argmax()]\n",
    "                \n",
    "                # If k_new == 0 (i.e. the topic is new)\n",
    "                if k_new == 0:\n",
    "                    \n",
    "                    # Add new dish and store as k_new \n",
    "                    for k_new, k in enumerate(using_k):\n",
    "                        if k_new != k: break\n",
    "                    else:\n",
    "                        k_new = len(using_k)\n",
    "                        if k_new >= len(n_kv):\n",
    "                            n_k = np.resize(n_k, k_new + 1)\n",
    "                            m_k = np.resize(m_k, k_new + 1)\n",
    "                            n_kv.append(None)\n",
    "                        assert k_new == using_k[-1] + 1\n",
    "                        assert k_new < len(n_kv)\n",
    "    \n",
    "                    using_k.insert(k_new, k_new)\n",
    "                    n_k[k_new] = beta * V\n",
    "                    m_k[k_new] = 0\n",
    "                    n_kv[k_new] = DefaultDict(beta)\n",
    "                    \n",
    "                assert k_new in using_k\n",
    "                \n",
    "                for t_new, t in enumerate(using_t[j]):\n",
    "                    if t_new != t: break\n",
    "                else:\n",
    "                    t_new = len(using_t[j])\n",
    "                    n_jt[j].resize(t_new+1)\n",
    "                    k_jt[j].resize(t_new+1)\n",
    "                    n_jtv[j].append(None)\n",
    "            \n",
    "                using_t[j].insert(t_new, t_new)\n",
    "                n_jt[j][t_new] = 0  # to make sure\n",
    "                n_jtv[j][t_new] = DefaultDict(0)\n",
    "            \n",
    "                k_jt[j][t_new] = k_new\n",
    "                \n",
    "                m_k[k_new] += 1\n",
    "                \n",
    "                m += 1\n",
    "            \n",
    "            assert t_new in using_t[j]\n",
    "            t_ji[j][i] = t_new\n",
    "            n_jt[j][t_new] += 1\n",
    "    \n",
    "            k_new = k_jt[j][t_new]\n",
    "            n_k[k_new] += 1\n",
    "    \n",
    "            v = x_ji[j][i]\n",
    "            n_kv[k_new][v] += 1\n",
    "            n_jtv[j][t_new][v] += 1\n",
    "            \n",
    "                \n",
    "    for j in range(M):\n",
    "        for t in using_t[j]:\n",
    "            if t != 0: \n",
    "                \"\"\"sampling k (dish=topic) from posterior\"\"\"\n",
    "    \n",
    "                #This makes the table leave from its dish and only the table counter decrease. The word counters (n_k and n_kv) stay.\n",
    "                \n",
    "                k = k_jt[j][t]\n",
    "                assert k > 0\n",
    "                assert m_k[k] > 0\n",
    "                \n",
    "                m_k[k] -= 1\n",
    "                m -= 1\n",
    "                if m_k[k] == 0:\n",
    "                    using_k.remove(k)\n",
    "                    k_jt[j][t] = 0\n",
    "                #\n",
    "                    \n",
    "                # sampling of k\n",
    "                p_k = calc_dish_posterior_t(j, t, n_k, n_jt, n_jtv)\n",
    "                \n",
    "                k_new = using_k[np.random.multinomial(1, p_k).argmax()]\n",
    "                \n",
    "\n",
    "                if k_new == 0:\n",
    "                    # Add new dish  \n",
    "                    for k_new, k in enumerate(using_k):\n",
    "                        if k_new != k: break\n",
    "                    else:\n",
    "                        k_new = len(using_k)\n",
    "                        if k_new >= len(n_kv):\n",
    "                            n_k = np.resize(n_k, k_new + 1)\n",
    "                            m_k = np.resize(m_k, k_new + 1)\n",
    "                            n_kv.append(None)\n",
    "                        assert k_new == using_k[-1] + 1\n",
    "                        assert k_new < len(n_kv)\n",
    "                \n",
    "                    using_k.insert(k_new, k_new)\n",
    "                    n_k[k_new] = beta * V\n",
    "                    m_k[k_new] = 0\n",
    "                    n_kv[k_new] = DefaultDict(beta)\n",
    "                    \n",
    "      \n",
    "                    \n",
    "                m += 1\n",
    "                m_k[k_new] += 1\n",
    "            \n",
    "                k_old = k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "                if k_new != k_old:\n",
    "                    k_jt[j][t] = k_new\n",
    "            \n",
    "                    n_jt2 = n_jt.copy()[j][t]\n",
    "                    if k_old != 0: n_k[k_old] -= n_jt2\n",
    "                    n_k[k_new] += n_jt2\n",
    "                    for v, n in n_jtv[j][t].items():\n",
    "                        if k_old != 0: n_kv[k_old][v] -= n\n",
    "                        n_kv[k_new][v] += n\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = DefaultDict(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing[3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity Checks ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sum of the words among all the tables in doc 0 equals the sum of the words in doc 0\n",
    "sum(n_jt[0]) == len(x_ji[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_jtv[0] (0 indexes first doc) is a list of dictionaries with each dictionary item giving the vocab index at a given table -- dict.keys() gives vocab index and dict.values() gives count \n",
    "# of that term at the table \n",
    "\n",
    "# This is the sum of words at the tables of doc 0.  It should be the same as n_jt[0][1:]\n",
    "[j for j in [sum(i.values()) for i in n_jtv[0] if i is not None] if j != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for j in [sum(i.values()) for i in n_jtv[0] if i is not None] if j != 0] == [h for h in n_jt[0] if h != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the total number of tables (m_k is the distribution of tables across the 26 topics)\n",
    "sum(m_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is also the total number of tables.  n_jt is the number of words at each table of each document.  This should be the same as sum(m_k) but it's off by 1 for some reason\n",
    "sum([len(g) for g in [[j for j in i if j!=0] for i in n_jt]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_k)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00049915, 0.00035676, 0.00024372, 0.00062696,\n",
       "       0.0006398 , 0.0001662 , 0.00062933, 0.00063012, 0.00063251,\n",
       "       0.00061996, 0.00063654, 0.00062539])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n_kv[5] for n_kv in n_kv] / n_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worddist(beta, n_kv, n_k, using_k):\n",
    "        \"\"\"return topic-word distribution without new topic\"\"\"\n",
    "        return [DefaultDict(beta / n_k[k]).update(\n",
    "            (v, n_kv / n_k[k]) for v, n_kv in n_kv[k].items())\n",
    "                for k in using_k if k != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002296096635719277"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word 59 appears .002296 times \n",
    "worddist(beta, n_kv, n_k, using_k)[0].get(59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of word 59 across the corpus \n",
    "freq59 = sum(beta/np.array([i.get(59, 1) for i in n_kv[1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in corpus\n",
    "tot_words = sum([len(i) for i in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020604539325165823"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(freq59/tot_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[DefaultDict(beta / n_k[k]).update((v, n_kv / n_k[k]) for v, n_kv in n_kv[k].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 4, 5, 6]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
