{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "import pandas as pd\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer as stemmer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk.stem\n",
    "\n",
    "from functools import reduce\n",
    "import collections \n",
    "\n",
    "\n",
    "# Special vocabulary module from shoyu\n",
    "import vocabulary_hdp as vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    '''Function that lemmatizes words in abstract by verbs'''\n",
    "    \n",
    "    return [stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v')) for w in doc.translate(str.maketrans('','', string.punctuation)).lower().split(' ')]\n",
    "\n",
    "def rm_stopwords_and_short_words(words, st_words):\n",
    "    '''Function removes stop words and those with length < 3'''\n",
    "    results = []\n",
    "    for i in words:\n",
    "        if not i in st_words and len(i)  > 3:\n",
    "            results.append(i)\n",
    "    return results\n",
    "\n",
    "def full_preprocess(doc, st_words):\n",
    "    '''Performs word lemmatization and stopword removal'''\n",
    "    return rm_stopwords_and_short_words(preprocess(doc), st_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ecoronado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tm_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_words = stopwords.words('english') # made this an argument of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place docs into dicitonary structure to be used in helper function below\n",
    "\n",
    "in_docs = {k: str(txt[0]) for k,txt in enumerate(df.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def tf(docs, st_words):\n",
    "    '''Function that calculates the term frequencies of word from an text-document paired dictionary input. \n",
    "       The output is a term frequency table '''\n",
    "    \n",
    "    # generate counts per document\n",
    "    counts = {k: Counter(full_preprocess(txt, st_words)) for k, txt in docs.items()}\n",
    "    \n",
    "    tf_df = pd.DataFrame.from_dict(counts).fillna(0).astype(int) # build pandas df, fill empty vals with 0s\n",
    "    \n",
    "    return(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created term frequency df with pre-processing functions\n",
    "tf_df = tf(in_docs, st_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in fewer than 3 abstracts and tokens that appear in more than half the abstracts \n",
    "\n",
    "filtered_df = tf_df[(tf_df.sum(axis=1) > 3)]\n",
    "filtered_df = filtered_df[(filtered_df.astype(bool).sum(axis=1) / tf_df.shape[1] < 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quickly build a dictionary based on filtered dataframe\n",
    "\n",
    "filt_words = pd.DataFrame.to_dict(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets non-zero words per document and creates nested list structure\n",
    "\n",
    "tokenized_df_ab  = [[word for word, cnt in words.items() if cnt!=0] for dkeys, words in filt_words.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = vocab.Vocabulary()\n",
    "docs = [voca.doc_to_ids(doc) for doc in tokenized_df_ab]\n",
    "beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper',\n",
       " 'high',\n",
       " 'interact',\n",
       " 'userfriend',\n",
       " 'lisp',\n",
       " 'program',\n",
       " 'introduc',\n",
       " 'perform',\n",
       " 'homogen',\n",
       " 'analysi',\n",
       " 'brief',\n",
       " 'introduct',\n",
       " 'techniqu',\n",
       " 'present',\n",
       " 'well',\n",
       " 'modif',\n",
       " 'presenc',\n",
       " 'miss',\n",
       " 'algorithm',\n",
       " 'discuss',\n",
       " 'overview',\n",
       " 'object',\n",
       " 'orient',\n",
       " 'code',\n",
       " 'produc',\n",
       " 'dialog',\n",
       " 'plot',\n",
       " 'order',\n",
       " 'demonstr',\n",
       " 'main',\n",
       " 'featur',\n",
       " 'small',\n",
       " 'larg',\n",
       " 'dataset',\n",
       " 'analyz',\n",
       " 'final',\n",
       " 'comparison',\n",
       " 'make',\n",
       " 'current',\n",
       " 'avail']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df_ab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_df = [full_preprocess(i,st_words) for i in df.abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words = reduce(lambda x,y: x+y, tokenized_df )\n",
    "#all_words =  list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out tokens that appear in fewer than 3 abstracts and tokens that appear in more than half the abstracts \n",
    "#all_words_counts = np.zeros(len(all_words))\n",
    "\n",
    "#for k,i in enumerate(all_words):\n",
    "#    for j in tokenized_df:\n",
    "#        if i in j:\n",
    "#            all_words_counts[k] += 1 \n",
    "            \n",
    "#word_counts_dict = list(zip(all_words, list(all_words_counts)))\n",
    "#word_counts_dict_ab = list(filter(lambda x: x[1] > 3, word_counts_dict))\n",
    "#word_counts_dict_ab2 = list(filter(lambda x: x[1] < len(tokenized_df)/2, word_counts_dict_ab))\n",
    "#final_dict = [i[0] for i in word_counts_dict_ab2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_df_ab = []\n",
    "#for i in tokenized_df:\n",
    "#    tokenized_df_ab.append([j for j in i if j in final_dict])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voca = vocab.Vocabulary()\n",
    "#docs = [voca.doc_to_ids(doc) for doc in tokenized_df_ab]\n",
    "#beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook data_prep.ipynb to script\n",
      "[NbConvertApp] Writing 2210 bytes to data_prep.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script 'data_prep.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
